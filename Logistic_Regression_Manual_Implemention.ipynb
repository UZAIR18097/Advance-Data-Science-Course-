{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression Manual Implemention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH82qhQ5_GXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Zd1IwyKmVe",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "75f5433a-04a6-435c-aca8-c1de564d31a7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a4847fa3-909b-4d88-be7a-d100d1127c43\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a4847fa3-909b-4d88-be7a-d100d1127c43\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ex4x.dat to ex4x.dat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvSAczovLPB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "x= np.genfromtxt(io.BytesIO(uploaded['ex4x.dat']))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH9Y3VDjLwxa",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4d344d99-9ff2-40ee-ceda-6419dddd1de9"
      },
      "source": [
        "uploaded = files.upload()\n",
        "y= np.genfromtxt(io.BytesIO(uploaded['ex4y.dat']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-969fd571-ef58-47ae-91c7-3e4dba29deed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-969fd571-ef58-47ae-91c7-3e4dba29deed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ex4y.dat to ex4y.dat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpAnOSzhL63E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "47a15755-bb8d-4f67-fb00-92a25d80486b"
      },
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80, 2)\n",
            "(80,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvYbjQKIMP01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = y.reshape([1,x.shape[0]])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VEv5aacNR9t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6d17b7d0-3488-4f5a-fed3-4af8b4e6d95a"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 80)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmooTBOrNTh5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "36b75c03-c83e-4c18-d79e-48ab88e42d30"
      },
      "source": [
        "m = x.shape[0]\n",
        "x = np.insert(x,0,np.ones(m),axis =1)\n",
        "x.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DhoWOlxOv-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#writing sigmoid function\n",
        "def sigmoid(x):\n",
        "  h =  1/(1+ np.exp(-x))\n",
        "  return h "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6O_FS5aPX-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "8bf4f36e-6f38-473c-abf3-5b5c6730f005"
      },
      "source": [
        "np.random.seed(2)\n",
        "theta = np.random.rand(x.shape[1],1)*0.01\n",
        "theta"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00435995],\n",
              "       [0.00025926],\n",
              "       [0.00549662]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6ska87GIDpx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "09073d2e-0574-4b28-b2b4-4a0bcb90f2da"
      },
      "source": [
        "print(theta.T.shape)\n",
        "print(x.T.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3)\n",
            "(3, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0cvdDmRSS0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f66396f-1f42-4ede-fa33-153f17fa25bf"
      },
      "source": [
        "\n",
        "#random intialization \n",
        "np.random.seed(2)\n",
        "theta = np.random.rand(x.shape[1],1)*0.01\n",
        "\n",
        "\n",
        "alpha = 0.001\n",
        "iter = 2000\n",
        "theta = np.random.rand(x.shape[1],1)*0.01\n",
        "cost_1 =np.zeros([1,iter])\n",
        "\n",
        " #looping for covergence\n",
        "for i in range(iter):\n",
        "  tmp = np.dot(theta.T,x.T)\n",
        "  h1 = sigmoid(tmp)\n",
        "  cost_1[0,i]=-(1/m)*np.sum((np.log10(h1)*y)+(np.log10(1-h1)*(1-y)))\n",
        "  grad = (1/m)*np.sum(((h1-y).T*x),axis =0 , keepdims=True)   #broadcasting\n",
        "  theta = theta - alpha*grad.T\n",
        "  \n",
        "  print(\"iteration\",i,\"Loss\",cost_1[0,i])\n",
        "  \n",
        "#Plotting  \n",
        "plt.plot(cost_1.T)\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 Loss 0.30081381633265847\n",
            "iteration 1 Loss 0.29871922637919957\n",
            "iteration 2 Loss 0.2979772386037354\n",
            "iteration 3 Loss 0.29754736932992837\n",
            "iteration 4 Loss 0.29721103264847937\n",
            "iteration 5 Loss 0.296906673909827\n",
            "iteration 6 Loss 0.2966187467308231\n",
            "iteration 7 Loss 0.296342834195452\n",
            "iteration 8 Loss 0.2960775055771751\n",
            "iteration 9 Loss 0.2958221036013137\n",
            "iteration 10 Loss 0.2955761819320817\n",
            "iteration 11 Loss 0.29533935977989806\n",
            "iteration 12 Loss 0.29511128309213597\n",
            "iteration 13 Loss 0.29489161473378767\n",
            "iteration 14 Loss 0.2946800313393081\n",
            "iteration 15 Loss 0.2944762223035829\n",
            "iteration 16 Loss 0.29427988912675523\n",
            "iteration 17 Loss 0.2940907449573219\n",
            "iteration 18 Loss 0.29390851414541425\n",
            "iteration 19 Loss 0.2937329318306094\n",
            "iteration 20 Loss 0.29356374353549913\n",
            "iteration 21 Loss 0.29340070477490104\n",
            "iteration 22 Loss 0.2932435806750172\n",
            "iteration 23 Loss 0.2930921456051438\n",
            "iteration 24 Loss 0.2929461828206404\n",
            "iteration 25 Loss 0.29280548411772445\n",
            "iteration 26 Loss 0.2926698494997035\n",
            "iteration 27 Loss 0.2925390868546602\n",
            "iteration 28 Loss 0.29241301164435923\n",
            "iteration 29 Loss 0.29229144660422074\n",
            "iteration 30 Loss 0.29217422145413\n",
            "iteration 31 Loss 0.29206117261985975\n",
            "iteration 32 Loss 0.2919521429648511\n",
            "iteration 33 Loss 0.29184698153209626\n",
            "iteration 34 Loss 0.29174554329584823\n",
            "iteration 35 Loss 0.2916476889228837\n",
            "iteration 36 Loss 0.29155328454303325\n",
            "iteration 37 Loss 0.29146220152869523\n",
            "iteration 38 Loss 0.29137431628304633\n",
            "iteration 39 Loss 0.2912895100366624\n",
            "iteration 40 Loss 0.29120766865226516\n",
            "iteration 41 Loss 0.2911286824373131\n",
            "iteration 42 Loss 0.29105244596415863\n",
            "iteration 43 Loss 0.29097885789749806\n",
            "iteration 44 Loss 0.2909078208288454\n",
            "iteration 45 Loss 0.2908392411177686\n",
            "iteration 46 Loss 0.29077302873963046\n",
            "iteration 47 Loss 0.29070909713958565\n",
            "iteration 48 Loss 0.2906473630925892\n",
            "iteration 49 Loss 0.29058774656918135\n",
            "iteration 50 Loss 0.2905301706068192\n",
            "iteration 51 Loss 0.2904745611865332\n",
            "iteration 52 Loss 0.290420847114694\n",
            "iteration 53 Loss 0.29036895990968176\n",
            "iteration 54 Loss 0.2903188336932579\n",
            "iteration 55 Loss 0.29027040508644536\n",
            "iteration 56 Loss 0.29022361310973244\n",
            "iteration 57 Loss 0.29017839908741916\n",
            "iteration 58 Loss 0.2901347065559347\n",
            "iteration 59 Loss 0.2900924811759588\n",
            "iteration 60 Loss 0.2900516706481889\n",
            "iteration 61 Loss 0.2900122246325979\n",
            "iteration 62 Loss 0.28997409467103724\n",
            "iteration 63 Loss 0.28993723411304173\n",
            "iteration 64 Loss 0.28990159804470245\n",
            "iteration 65 Loss 0.28986714322047663\n",
            "iteration 66 Loss 0.28983382799780977\n",
            "iteration 67 Loss 0.28980161227445117\n",
            "iteration 68 Loss 0.2897704574283476\n",
            "iteration 69 Loss 0.28974032626000557\n",
            "iteration 70 Loss 0.2897111829372175\n",
            "iteration 71 Loss 0.28968299294205035\n",
            "iteration 72 Loss 0.2896557230200018\n",
            "iteration 73 Loss 0.2896293411312299\n",
            "iteration 74 Loss 0.2896038164037695\n",
            "iteration 75 Loss 0.2895791190886502\n",
            "iteration 76 Loss 0.2895552205168354\n",
            "iteration 77 Loss 0.2895320930579048\n",
            "iteration 78 Loss 0.28950971008040655\n",
            "iteration 79 Loss 0.28948804591380833\n",
            "iteration 80 Loss 0.2894670758119801\n",
            "iteration 81 Loss 0.2894467759181426\n",
            "iteration 82 Loss 0.2894271232312208\n",
            "iteration 83 Loss 0.28940809557354313\n",
            "iteration 84 Loss 0.2893896715598287\n",
            "iteration 85 Loss 0.28937183056740984\n",
            "iteration 86 Loss 0.289354552707637\n",
            "iteration 87 Loss 0.2893378187984169\n",
            "iteration 88 Loss 0.2893216103378368\n",
            "iteration 89 Loss 0.2893059094788293\n",
            "iteration 90 Loss 0.28929069900483384\n",
            "iteration 91 Loss 0.2892759623064149\n",
            "iteration 92 Loss 0.2892616833587951\n",
            "iteration 93 Loss 0.2892478467002678\n",
            "iteration 94 Loss 0.28923443741145033\n",
            "iteration 95 Loss 0.28922144109534526\n",
            "iteration 96 Loss 0.2892088438581748\n",
            "iteration 97 Loss 0.28919663229095854\n",
            "iteration 98 Loss 0.28918479345180165\n",
            "iteration 99 Loss 0.28917331484886644\n",
            "iteration 100 Loss 0.28916218442399905\n",
            "iteration 101 Loss 0.28915139053698347\n",
            "iteration 102 Loss 0.2891409219503986\n",
            "iteration 103 Loss 0.28913076781505404\n",
            "iteration 104 Loss 0.28912091765598014\n",
            "iteration 105 Loss 0.2891113613589506\n",
            "iteration 106 Loss 0.28910208915751684\n",
            "iteration 107 Loss 0.28909309162053193\n",
            "iteration 108 Loss 0.2890843596401464\n",
            "iteration 109 Loss 0.2890758844202554\n",
            "iteration 110 Loss 0.28906765746538116\n",
            "iteration 111 Loss 0.28905967056997056\n",
            "iteration 112 Loss 0.2890519158080947\n",
            "iteration 113 Loss 0.28904438552353195\n",
            "iteration 114 Loss 0.2890370723202209\n",
            "iteration 115 Loss 0.2890299690530677\n",
            "iteration 116 Loss 0.2890230688190946\n",
            "iteration 117 Loss 0.2890163649489157\n",
            "iteration 118 Loss 0.28900985099852744\n",
            "iteration 119 Loss 0.2890035207414016\n",
            "iteration 120 Loss 0.2889973681608693\n",
            "iteration 121 Loss 0.28899138744278374\n",
            "iteration 122 Loss 0.28898557296845173\n",
            "iteration 123 Loss 0.2889799193078237\n",
            "iteration 124 Loss 0.288974421212932\n",
            "iteration 125 Loss 0.28896907361156693\n",
            "iteration 126 Loss 0.2889638716011838\n",
            "iteration 127 Loss 0.2889588104430298\n",
            "iteration 128 Loss 0.2889538855564833\n",
            "iteration 129 Loss 0.28894909251359757\n",
            "iteration 130 Loss 0.2889444270338412\n",
            "iteration 131 Loss 0.28893988497902695\n",
            "iteration 132 Loss 0.2889354623484234\n",
            "iteration 133 Loss 0.28893115527404056\n",
            "iteration 134 Loss 0.28892696001608514\n",
            "iteration 135 Loss 0.2889228729585765\n",
            "iteration 136 Loss 0.2889188906051201\n",
            "iteration 137 Loss 0.2889150095748303\n",
            "iteration 138 Loss 0.2889112265983978\n",
            "iteration 139 Loss 0.2889075385142966\n",
            "iteration 140 Loss 0.28890394226512484\n",
            "iteration 141 Loss 0.2889004348940747\n",
            "iteration 142 Loss 0.28889701354152675\n",
            "iteration 143 Loss 0.28889367544176386\n",
            "iteration 144 Loss 0.28889041791980036\n",
            "iteration 145 Loss 0.28888723838832275\n",
            "iteration 146 Loss 0.2888841343447366\n",
            "iteration 147 Loss 0.2888811033683173\n",
            "iteration 148 Loss 0.28887814311745963\n",
            "iteration 149 Loss 0.2888752513270233\n",
            "iteration 150 Loss 0.28887242580577066\n",
            "iteration 151 Loss 0.2888696644338931\n",
            "iteration 152 Loss 0.2888669651606233\n",
            "iteration 153 Loss 0.28886432600193007\n",
            "iteration 154 Loss 0.2888617450382925\n",
            "iteration 155 Loss 0.288859220412551\n",
            "iteration 156 Loss 0.28885675032783237\n",
            "iteration 157 Loss 0.28885433304554614\n",
            "iteration 158 Loss 0.28885196688344933\n",
            "iteration 159 Loss 0.288849650213778\n",
            "iteration 160 Loss 0.2888473814614425\n",
            "iteration 161 Loss 0.2888451591022841\n",
            "iteration 162 Loss 0.28884298166139155\n",
            "iteration 163 Loss 0.2888408477114749\n",
            "iteration 164 Loss 0.2888387558712941\n",
            "iteration 165 Loss 0.2888367048041417\n",
            "iteration 166 Loss 0.28883469321637684\n",
            "iteration 167 Loss 0.2888327198560082\n",
            "iteration 168 Loss 0.28883078351132613\n",
            "iteration 169 Loss 0.2888288830095799\n",
            "iteration 170 Loss 0.28882701721569987\n",
            "iteration 171 Loss 0.28882518503106347\n",
            "iteration 172 Loss 0.2888233853923016\n",
            "iteration 173 Loss 0.28882161727014566\n",
            "iteration 174 Loss 0.28881987966831346\n",
            "iteration 175 Loss 0.2888181716224323\n",
            "iteration 176 Loss 0.288816492198998\n",
            "iteration 177 Loss 0.2888148404943689\n",
            "iteration 178 Loss 0.28881321563379364\n",
            "iteration 179 Loss 0.28881161677047074\n",
            "iteration 180 Loss 0.2888100430846406\n",
            "iteration 181 Loss 0.28880849378270657\n",
            "iteration 182 Loss 0.288806968096386\n",
            "iteration 183 Loss 0.2888054652818896\n",
            "iteration 184 Loss 0.288803984619127\n",
            "iteration 185 Loss 0.2888025254109401\n",
            "iteration 186 Loss 0.28880108698236057\n",
            "iteration 187 Loss 0.28879966867989293\n",
            "iteration 188 Loss 0.28879826987081997\n",
            "iteration 189 Loss 0.28879688994253316\n",
            "iteration 190 Loss 0.2887955283018827\n",
            "iteration 191 Loss 0.2887941843745509\n",
            "iteration 192 Loss 0.28879285760444556\n",
            "iteration 193 Loss 0.2887915474531126\n",
            "iteration 194 Loss 0.28879025339916925\n",
            "iteration 195 Loss 0.28878897493775507\n",
            "iteration 196 Loss 0.28878771158000077\n",
            "iteration 197 Loss 0.2887864628525152\n",
            "iteration 198 Loss 0.28878522829688835\n",
            "iteration 199 Loss 0.2887840074692112\n",
            "iteration 200 Loss 0.28878279993961076\n",
            "iteration 201 Loss 0.2887816052918008\n",
            "iteration 202 Loss 0.2887804231226468\n",
            "iteration 203 Loss 0.2887792530417452\n",
            "iteration 204 Loss 0.28877809467101645\n",
            "iteration 205 Loss 0.28877694764431133\n",
            "iteration 206 Loss 0.2887758116070298\n",
            "iteration 207 Loss 0.2887746862157526\n",
            "iteration 208 Loss 0.2887735711378842\n",
            "iteration 209 Loss 0.2887724660513084\n",
            "iteration 210 Loss 0.2887713706440538\n",
            "iteration 211 Loss 0.288770284613971\n",
            "iteration 212 Loss 0.28876920766841996\n",
            "iteration 213 Loss 0.2887681395239675\n",
            "iteration 214 Loss 0.28876707990609457\n",
            "iteration 215 Loss 0.28876602854891265\n",
            "iteration 216 Loss 0.2887649851948901\n",
            "iteration 217 Loss 0.28876394959458634\n",
            "iteration 218 Loss 0.28876292150639554\n",
            "iteration 219 Loss 0.2887619006962976\n",
            "iteration 220 Loss 0.2887608869376181\n",
            "iteration 221 Loss 0.2887598800107954\n",
            "iteration 222 Loss 0.288758879703155\n",
            "iteration 223 Loss 0.2887578858086919\n",
            "iteration 224 Loss 0.2887568981278594\n",
            "iteration 225 Loss 0.2887559164673647\n",
            "iteration 226 Loss 0.28875494063997115\n",
            "iteration 227 Loss 0.28875397046430695\n",
            "iteration 228 Loss 0.28875300576467994\n",
            "iteration 229 Loss 0.28875204637089774\n",
            "iteration 230 Loss 0.2887510921180947\n",
            "iteration 231 Loss 0.2887501428465636\n",
            "iteration 232 Loss 0.28874919840159274\n",
            "iteration 233 Loss 0.28874825863330866\n",
            "iteration 234 Loss 0.28874732339652354\n",
            "iteration 235 Loss 0.28874639255058765\n",
            "iteration 236 Loss 0.28874546595924633\n",
            "iteration 237 Loss 0.28874454349050155\n",
            "iteration 238 Loss 0.28874362501647816\n",
            "iteration 239 Loss 0.28874271041329386\n",
            "iteration 240 Loss 0.2887417995609339\n",
            "iteration 241 Loss 0.2887408923431292\n",
            "iteration 242 Loss 0.2887399886472392\n",
            "iteration 243 Loss 0.2887390883641368\n",
            "iteration 244 Loss 0.2887381913880992\n",
            "iteration 245 Loss 0.2887372976167001\n",
            "iteration 246 Loss 0.28873640695070657\n",
            "iteration 247 Loss 0.28873551929397917\n",
            "iteration 248 Loss 0.2887346345533742\n",
            "iteration 249 Loss 0.28873375263865014\n",
            "iteration 250 Loss 0.28873287346237697\n",
            "iteration 251 Loss 0.2887319969398472\n",
            "iteration 252 Loss 0.28873112298899145\n",
            "iteration 253 Loss 0.2887302515302955\n",
            "iteration 254 Loss 0.28872938248671987\n",
            "iteration 255 Loss 0.2887285157836229\n",
            "iteration 256 Loss 0.2887276513486855\n",
            "iteration 257 Loss 0.2887267891118384\n",
            "iteration 258 Loss 0.2887259290051919\n",
            "iteration 259 Loss 0.2887250709629674\n",
            "iteration 260 Loss 0.2887242149214321\n",
            "iteration 261 Loss 0.288723360818834\n",
            "iteration 262 Loss 0.28872250859534104\n",
            "iteration 263 Loss 0.28872165819298057\n",
            "iteration 264 Loss 0.28872080955558116\n",
            "iteration 265 Loss 0.2887199626287168\n",
            "iteration 266 Loss 0.2887191173596519\n",
            "iteration 267 Loss 0.28871827369728903\n",
            "iteration 268 Loss 0.28871743159211744\n",
            "iteration 269 Loss 0.2887165909961637\n",
            "iteration 270 Loss 0.2887157518629437\n",
            "iteration 271 Loss 0.28871491414741607\n",
            "iteration 272 Loss 0.2887140778059378\n",
            "iteration 273 Loss 0.28871324279621957\n",
            "iteration 274 Loss 0.2887124090772848\n",
            "iteration 275 Loss 0.2887115766094278\n",
            "iteration 276 Loss 0.2887107453541744\n",
            "iteration 277 Loss 0.28870991527424417\n",
            "iteration 278 Loss 0.2887090863335122\n",
            "iteration 279 Loss 0.2887082584969742\n",
            "iteration 280 Loss 0.28870743173071073\n",
            "iteration 281 Loss 0.2887066060018542\n",
            "iteration 282 Loss 0.2887057812785555\n",
            "iteration 283 Loss 0.2887049575299526\n",
            "iteration 284 Loss 0.2887041347261398\n",
            "iteration 285 Loss 0.2887033128381384\n",
            "iteration 286 Loss 0.2887024918378667\n",
            "iteration 287 Loss 0.2887016716981137\n",
            "iteration 288 Loss 0.2887008523925108\n",
            "iteration 289 Loss 0.2887000338955061\n",
            "iteration 290 Loss 0.28869921618233896\n",
            "iteration 291 Loss 0.28869839922901547\n",
            "iteration 292 Loss 0.28869758301228465\n",
            "iteration 293 Loss 0.28869676750961526\n",
            "iteration 294 Loss 0.28869595269917336\n",
            "iteration 295 Loss 0.2886951385598009\n",
            "iteration 296 Loss 0.28869432507099474\n",
            "iteration 297 Loss 0.2886935122128859\n",
            "iteration 298 Loss 0.2886926999662207\n",
            "iteration 299 Loss 0.2886918883123406\n",
            "iteration 300 Loss 0.28869107723316473\n",
            "iteration 301 Loss 0.288690266711171\n",
            "iteration 302 Loss 0.28868945672937996\n",
            "iteration 303 Loss 0.28868864727133675\n",
            "iteration 304 Loss 0.2886878383210957\n",
            "iteration 305 Loss 0.288687029863204\n",
            "iteration 306 Loss 0.28868622188268683\n",
            "iteration 307 Loss 0.28868541436503226\n",
            "iteration 308 Loss 0.28868460729617695\n",
            "iteration 309 Loss 0.2886838006624924\n",
            "iteration 310 Loss 0.28868299445077134\n",
            "iteration 311 Loss 0.2886821886482146\n",
            "iteration 312 Loss 0.28868138324241865\n",
            "iteration 313 Loss 0.2886805782213634\n",
            "iteration 314 Loss 0.2886797735733999\n",
            "iteration 315 Loss 0.2886789692872394\n",
            "iteration 316 Loss 0.2886781653519417\n",
            "iteration 317 Loss 0.2886773617569048\n",
            "iteration 318 Loss 0.28867655849185403\n",
            "iteration 319 Loss 0.28867575554683217\n",
            "iteration 320 Loss 0.28867495291218964\n",
            "iteration 321 Loss 0.2886741505785748\n",
            "iteration 322 Loss 0.28867334853692483\n",
            "iteration 323 Loss 0.2886725467784567\n",
            "iteration 324 Loss 0.2886717452946588\n",
            "iteration 325 Loss 0.2886709440772821\n",
            "iteration 326 Loss 0.28867014311833245\n",
            "iteration 327 Loss 0.2886693424100625\n",
            "iteration 328 Loss 0.2886685419449639\n",
            "iteration 329 Loss 0.2886677417157602\n",
            "iteration 330 Loss 0.28866694171539964\n",
            "iteration 331 Loss 0.2886661419370479\n",
            "iteration 332 Loss 0.28866534237408187\n",
            "iteration 333 Loss 0.28866454302008265\n",
            "iteration 334 Loss 0.2886637438688296\n",
            "iteration 335 Loss 0.28866294491429373\n",
            "iteration 336 Loss 0.28866214615063257\n",
            "iteration 337 Loss 0.2886613475721835\n",
            "iteration 338 Loss 0.28866054917345874\n",
            "iteration 339 Loss 0.28865975094914004\n",
            "iteration 340 Loss 0.2886589528940729\n",
            "iteration 341 Loss 0.2886581550032621\n",
            "iteration 342 Loss 0.2886573572718663\n",
            "iteration 343 Loss 0.2886565596951937\n",
            "iteration 344 Loss 0.2886557622686972\n",
            "iteration 345 Loss 0.2886549649879698\n",
            "iteration 346 Loss 0.2886541678487404\n",
            "iteration 347 Loss 0.28865337084686987\n",
            "iteration 348 Loss 0.28865257397834654\n",
            "iteration 349 Loss 0.2886517772392826\n",
            "iteration 350 Loss 0.28865098062591\n",
            "iteration 351 Loss 0.2886501841345768\n",
            "iteration 352 Loss 0.288649387761744\n",
            "iteration 353 Loss 0.2886485915039813\n",
            "iteration 354 Loss 0.28864779535796464\n",
            "iteration 355 Loss 0.28864699932047194\n",
            "iteration 356 Loss 0.2886462033883809\n",
            "iteration 357 Loss 0.28864540755866497\n",
            "iteration 358 Loss 0.2886446118283915\n",
            "iteration 359 Loss 0.28864381619471785\n",
            "iteration 360 Loss 0.28864302065488906\n",
            "iteration 361 Loss 0.2886422252062352\n",
            "iteration 362 Loss 0.28864142984616853\n",
            "iteration 363 Loss 0.28864063457218114\n",
            "iteration 364 Loss 0.28863983938184234\n",
            "iteration 365 Loss 0.2886390442727964\n",
            "iteration 366 Loss 0.2886382492427603\n",
            "iteration 367 Loss 0.2886374542895211\n",
            "iteration 368 Loss 0.28863665941093436\n",
            "iteration 369 Loss 0.28863586460492147\n",
            "iteration 370 Loss 0.28863506986946813\n",
            "iteration 371 Loss 0.28863427520262175\n",
            "iteration 372 Loss 0.28863348060249033\n",
            "iteration 373 Loss 0.28863268606723985\n",
            "iteration 374 Loss 0.2886318915950929\n",
            "iteration 375 Loss 0.2886310971843269\n",
            "iteration 376 Loss 0.2886303028332722\n",
            "iteration 377 Loss 0.2886295085403107\n",
            "iteration 378 Loss 0.2886287143038742\n",
            "iteration 379 Loss 0.2886279201224426\n",
            "iteration 380 Loss 0.2886271259945429\n",
            "iteration 381 Loss 0.2886263319187471\n",
            "iteration 382 Loss 0.2886255378936716\n",
            "iteration 383 Loss 0.2886247439179752\n",
            "iteration 384 Loss 0.28862394999035806\n",
            "iteration 385 Loss 0.2886231561095604\n",
            "iteration 386 Loss 0.28862236227436117\n",
            "iteration 387 Loss 0.2886215684835769\n",
            "iteration 388 Loss 0.2886207747360607\n",
            "iteration 389 Loss 0.288619981030701\n",
            "iteration 390 Loss 0.28861918736642034\n",
            "iteration 391 Loss 0.28861839374217446\n",
            "iteration 392 Loss 0.2886176001569514\n",
            "iteration 393 Loss 0.2886168066097703\n",
            "iteration 394 Loss 0.2886160130996804\n",
            "iteration 395 Loss 0.28861521962576026\n",
            "iteration 396 Loss 0.28861442618711686\n",
            "iteration 397 Loss 0.2886136327828847\n",
            "iteration 398 Loss 0.28861283941222504\n",
            "iteration 399 Loss 0.2886120460743246\n",
            "iteration 400 Loss 0.28861125276839544\n",
            "iteration 401 Loss 0.28861045949367387\n",
            "iteration 402 Loss 0.2886096662494196\n",
            "iteration 403 Loss 0.28860887303491517\n",
            "iteration 404 Loss 0.28860807984946524\n",
            "iteration 405 Loss 0.2886072866923958\n",
            "iteration 406 Loss 0.28860649356305385\n",
            "iteration 407 Loss 0.288605700460806\n",
            "iteration 408 Loss 0.2886049073850389\n",
            "iteration 409 Loss 0.28860411433515776\n",
            "iteration 410 Loss 0.28860332131058614\n",
            "iteration 411 Loss 0.2886025283107654\n",
            "iteration 412 Loss 0.2886017353351541\n",
            "iteration 413 Loss 0.28860094238322737\n",
            "iteration 414 Loss 0.2886001494544767\n",
            "iteration 415 Loss 0.288599356548409\n",
            "iteration 416 Loss 0.28859856366454645\n",
            "iteration 417 Loss 0.28859777080242605\n",
            "iteration 418 Loss 0.2885969779615989\n",
            "iteration 419 Loss 0.28859618514162994\n",
            "iteration 420 Loss 0.2885953923420976\n",
            "iteration 421 Loss 0.2885945995625931\n",
            "iteration 422 Loss 0.28859380680272045\n",
            "iteration 423 Loss 0.28859301406209575\n",
            "iteration 424 Loss 0.28859222134034684\n",
            "iteration 425 Loss 0.2885914286371132\n",
            "iteration 426 Loss 0.2885906359520452\n",
            "iteration 427 Loss 0.28858984328480414\n",
            "iteration 428 Loss 0.28858905063506163\n",
            "iteration 429 Loss 0.2885882580024994\n",
            "iteration 430 Loss 0.28858746538680896\n",
            "iteration 431 Loss 0.2885866727876915\n",
            "iteration 432 Loss 0.2885858802048572\n",
            "iteration 433 Loss 0.28858508763802515\n",
            "iteration 434 Loss 0.28858429508692324\n",
            "iteration 435 Loss 0.2885835025512875\n",
            "iteration 436 Loss 0.2885827100308624\n",
            "iteration 437 Loss 0.2885819175254\n",
            "iteration 438 Loss 0.2885811250346601\n",
            "iteration 439 Loss 0.28858033255840987\n",
            "iteration 440 Loss 0.2885795400964236\n",
            "iteration 441 Loss 0.2885787476484826\n",
            "iteration 442 Loss 0.28857795521437507\n",
            "iteration 443 Loss 0.2885771627938953\n",
            "iteration 444 Loss 0.2885763703868442\n",
            "iteration 445 Loss 0.28857557799302874\n",
            "iteration 446 Loss 0.28857478561226196\n",
            "iteration 447 Loss 0.28857399324436245\n",
            "iteration 448 Loss 0.2885732008891544\n",
            "iteration 449 Loss 0.2885724085464676\n",
            "iteration 450 Loss 0.28857161621613686\n",
            "iteration 451 Loss 0.2885708238980022\n",
            "iteration 452 Loss 0.2885700315919084\n",
            "iteration 453 Loss 0.2885692392977053\n",
            "iteration 454 Loss 0.28856844701524714\n",
            "iteration 455 Loss 0.28856765474439267\n",
            "iteration 456 Loss 0.288566862485005\n",
            "iteration 457 Loss 0.2885660702369516\n",
            "iteration 458 Loss 0.2885652780001036\n",
            "iteration 459 Loss 0.28856448577433663\n",
            "iteration 460 Loss 0.2885636935595297\n",
            "iteration 461 Loss 0.2885629013555659\n",
            "iteration 462 Loss 0.2885621091623316\n",
            "iteration 463 Loss 0.28856131697971693\n",
            "iteration 464 Loss 0.2885605248076152\n",
            "iteration 465 Loss 0.288559732645923\n",
            "iteration 466 Loss 0.2885589404945403\n",
            "iteration 467 Loss 0.28855814835337\n",
            "iteration 468 Loss 0.288557356222318\n",
            "iteration 469 Loss 0.28855656410129304\n",
            "iteration 470 Loss 0.28855577199020677\n",
            "iteration 471 Loss 0.28855497988897355\n",
            "iteration 472 Loss 0.28855418779751035\n",
            "iteration 473 Loss 0.28855339571573646\n",
            "iteration 474 Loss 0.2885526036435742\n",
            "iteration 475 Loss 0.2885518115809477\n",
            "iteration 476 Loss 0.28855101952778384\n",
            "iteration 477 Loss 0.2885502274840115\n",
            "iteration 478 Loss 0.2885494354495618\n",
            "iteration 479 Loss 0.288548643424368\n",
            "iteration 480 Loss 0.2885478514083654\n",
            "iteration 481 Loss 0.28854705940149145\n",
            "iteration 482 Loss 0.28854626740368516\n",
            "iteration 483 Loss 0.28854547541488784\n",
            "iteration 484 Loss 0.28854468343504225\n",
            "iteration 485 Loss 0.28854389146409304\n",
            "iteration 486 Loss 0.28854309950198664\n",
            "iteration 487 Loss 0.28854230754867105\n",
            "iteration 488 Loss 0.2885415156040959\n",
            "iteration 489 Loss 0.28854072366821226\n",
            "iteration 490 Loss 0.28853993174097287\n",
            "iteration 491 Loss 0.28853913982233176\n",
            "iteration 492 Loss 0.28853834791224453\n",
            "iteration 493 Loss 0.288537556010668\n",
            "iteration 494 Loss 0.28853676411756046\n",
            "iteration 495 Loss 0.2885359722328814\n",
            "iteration 496 Loss 0.2885351803565914\n",
            "iteration 497 Loss 0.28853438848865265\n",
            "iteration 498 Loss 0.28853359662902806\n",
            "iteration 499 Loss 0.288532804777682\n",
            "iteration 500 Loss 0.28853201293457975\n",
            "iteration 501 Loss 0.28853122109968776\n",
            "iteration 502 Loss 0.2885304292729734\n",
            "iteration 503 Loss 0.28852963745440513\n",
            "iteration 504 Loss 0.2885288456439525\n",
            "iteration 505 Loss 0.28852805384158564\n",
            "iteration 506 Loss 0.288527262047276\n",
            "iteration 507 Loss 0.28852647026099554\n",
            "iteration 508 Loss 0.28852567848271743\n",
            "iteration 509 Loss 0.2885248867124154\n",
            "iteration 510 Loss 0.28852409495006404\n",
            "iteration 511 Loss 0.28852330319563885\n",
            "iteration 512 Loss 0.2885225114491159\n",
            "iteration 513 Loss 0.2885217197104722\n",
            "iteration 514 Loss 0.2885209279796852\n",
            "iteration 515 Loss 0.2885201362567333\n",
            "iteration 516 Loss 0.2885193445415954\n",
            "iteration 517 Loss 0.28851855283425115\n",
            "iteration 518 Loss 0.28851776113468075\n",
            "iteration 519 Loss 0.28851696944286503\n",
            "iteration 520 Loss 0.2885161777587854\n",
            "iteration 521 Loss 0.28851538608242383\n",
            "iteration 522 Loss 0.2885145944137629\n",
            "iteration 523 Loss 0.2885138027527856\n",
            "iteration 524 Loss 0.28851301109947575\n",
            "iteration 525 Loss 0.28851221945381716\n",
            "iteration 526 Loss 0.28851142781579464\n",
            "iteration 527 Loss 0.288510636185393\n",
            "iteration 528 Loss 0.28850984456259804\n",
            "iteration 529 Loss 0.28850905294739554\n",
            "iteration 530 Loss 0.288508261339772\n",
            "iteration 531 Loss 0.28850746973971403\n",
            "iteration 532 Loss 0.2885066781472091\n",
            "iteration 533 Loss 0.2885058865622446\n",
            "iteration 534 Loss 0.2885050949848086\n",
            "iteration 535 Loss 0.2885043034148894\n",
            "iteration 536 Loss 0.28850351185247575\n",
            "iteration 537 Loss 0.2885027202975567\n",
            "iteration 538 Loss 0.2885019287501216\n",
            "iteration 539 Loss 0.2885011372101602\n",
            "iteration 540 Loss 0.2885003456776625\n",
            "iteration 541 Loss 0.28849955415261874\n",
            "iteration 542 Loss 0.2884987626350197\n",
            "iteration 543 Loss 0.28849797112485626\n",
            "iteration 544 Loss 0.28849717962211957\n",
            "iteration 545 Loss 0.28849638812680106\n",
            "iteration 546 Loss 0.28849559663889246\n",
            "iteration 547 Loss 0.28849480515838577\n",
            "iteration 548 Loss 0.28849401368527317\n",
            "iteration 549 Loss 0.2884932222195471\n",
            "iteration 550 Loss 0.2884924307612004\n",
            "iteration 551 Loss 0.2884916393102257\n",
            "iteration 552 Loss 0.2884908478666164\n",
            "iteration 553 Loss 0.2884900564303656\n",
            "iteration 554 Loss 0.28848926500146704\n",
            "iteration 555 Loss 0.2884884735799143\n",
            "iteration 556 Loss 0.28848768216570136\n",
            "iteration 557 Loss 0.28848689075882233\n",
            "iteration 558 Loss 0.28848609935927144\n",
            "iteration 559 Loss 0.2884853079670433\n",
            "iteration 560 Loss 0.2884845165821324\n",
            "iteration 561 Loss 0.2884837252045335\n",
            "iteration 562 Loss 0.28848293383424173\n",
            "iteration 563 Loss 0.28848214247125203\n",
            "iteration 564 Loss 0.2884813511155597\n",
            "iteration 565 Loss 0.2884805597671602\n",
            "iteration 566 Loss 0.288479768426049\n",
            "iteration 567 Loss 0.28847897709222176\n",
            "iteration 568 Loss 0.28847818576567436\n",
            "iteration 569 Loss 0.2884773944464025\n",
            "iteration 570 Loss 0.2884766031344026\n",
            "iteration 571 Loss 0.2884758118296706\n",
            "iteration 572 Loss 0.28847502053220275\n",
            "iteration 573 Loss 0.28847422924199556\n",
            "iteration 574 Loss 0.2884734379590455\n",
            "iteration 575 Loss 0.28847264668334927\n",
            "iteration 576 Loss 0.2884718554149034\n",
            "iteration 577 Loss 0.2884710641537049\n",
            "iteration 578 Loss 0.2884702728997505\n",
            "iteration 579 Loss 0.28846948165303743\n",
            "iteration 580 Loss 0.28846869041356255\n",
            "iteration 581 Loss 0.2884678991813232\n",
            "iteration 582 Loss 0.2884671079563165\n",
            "iteration 583 Loss 0.28846631673854006\n",
            "iteration 584 Loss 0.28846552552799104\n",
            "iteration 585 Loss 0.28846473432466707\n",
            "iteration 586 Loss 0.2884639431285656\n",
            "iteration 587 Loss 0.28846315193968447\n",
            "iteration 588 Loss 0.2884623607580214\n",
            "iteration 589 Loss 0.28846156958357394\n",
            "iteration 590 Loss 0.28846077841634027\n",
            "iteration 591 Loss 0.28845998725631805\n",
            "iteration 592 Loss 0.2884591961035054\n",
            "iteration 593 Loss 0.2884584049579003\n",
            "iteration 594 Loss 0.2884576138195009\n",
            "iteration 595 Loss 0.2884568226883053\n",
            "iteration 596 Loss 0.28845603156431177\n",
            "iteration 597 Loss 0.2884552404475185\n",
            "iteration 598 Loss 0.2884544493379239\n",
            "iteration 599 Loss 0.2884536582355263\n",
            "iteration 600 Loss 0.2884528671403242\n",
            "iteration 601 Loss 0.2884520760523159\n",
            "iteration 602 Loss 0.28845128497149986\n",
            "iteration 603 Loss 0.28845049389787486\n",
            "iteration 604 Loss 0.28844970283143934\n",
            "iteration 605 Loss 0.28844891177219195\n",
            "iteration 606 Loss 0.28844812072013143\n",
            "iteration 607 Loss 0.28844732967525644\n",
            "iteration 608 Loss 0.28844653863756575\n",
            "iteration 609 Loss 0.28844574760705816\n",
            "iteration 610 Loss 0.28844495658373237\n",
            "iteration 611 Loss 0.28844416556758745\n",
            "iteration 612 Loss 0.2884433745586222\n",
            "iteration 613 Loss 0.28844258355683533\n",
            "iteration 614 Loss 0.2884417925622262\n",
            "iteration 615 Loss 0.28844100157479347\n",
            "iteration 616 Loss 0.28844021059453623\n",
            "iteration 617 Loss 0.2884394196214536\n",
            "iteration 618 Loss 0.28843862865554465\n",
            "iteration 619 Loss 0.28843783769680836\n",
            "iteration 620 Loss 0.28843704674524384\n",
            "iteration 621 Loss 0.28843625580085036\n",
            "iteration 622 Loss 0.2884354648636271\n",
            "iteration 623 Loss 0.2884346739335732\n",
            "iteration 624 Loss 0.2884338830106878\n",
            "iteration 625 Loss 0.28843309209497026\n",
            "iteration 626 Loss 0.28843230118641977\n",
            "iteration 627 Loss 0.28843151028503566\n",
            "iteration 628 Loss 0.2884307193908171\n",
            "iteration 629 Loss 0.28842992850376364\n",
            "iteration 630 Loss 0.28842913762387445\n",
            "iteration 631 Loss 0.2884283467511489\n",
            "iteration 632 Loss 0.2884275558855864\n",
            "iteration 633 Loss 0.28842676502718634\n",
            "iteration 634 Loss 0.2884259741759482\n",
            "iteration 635 Loss 0.2884251833318713\n",
            "iteration 636 Loss 0.28842439249495505\n",
            "iteration 637 Loss 0.288423601665199\n",
            "iteration 638 Loss 0.2884228108426026\n",
            "iteration 639 Loss 0.2884220200271653\n",
            "iteration 640 Loss 0.2884212292188867\n",
            "iteration 641 Loss 0.28842043841776616\n",
            "iteration 642 Loss 0.2884196476238033\n",
            "iteration 643 Loss 0.2884188568369976\n",
            "iteration 644 Loss 0.2884180660573487\n",
            "iteration 645 Loss 0.28841727528485617\n",
            "iteration 646 Loss 0.2884164845195195\n",
            "iteration 647 Loss 0.2884156937613383\n",
            "iteration 648 Loss 0.2884149030103122\n",
            "iteration 649 Loss 0.28841411226644065\n",
            "iteration 650 Loss 0.2884133215297235\n",
            "iteration 651 Loss 0.2884125308001602\n",
            "iteration 652 Loss 0.28841174007775056\n",
            "iteration 653 Loss 0.28841094936249395\n",
            "iteration 654 Loss 0.28841015865439035\n",
            "iteration 655 Loss 0.28840936795343913\n",
            "iteration 656 Loss 0.28840857725964025\n",
            "iteration 657 Loss 0.2884077865729931\n",
            "iteration 658 Loss 0.28840699589349755\n",
            "iteration 659 Loss 0.28840620522115323\n",
            "iteration 660 Loss 0.2884054145559599\n",
            "iteration 661 Loss 0.28840462389791716\n",
            "iteration 662 Loss 0.28840383324702484\n",
            "iteration 663 Loss 0.2884030426032826\n",
            "iteration 664 Loss 0.28840225196669017\n",
            "iteration 665 Loss 0.28840146133724726\n",
            "iteration 666 Loss 0.28840067071495373\n",
            "iteration 667 Loss 0.2883998800998092\n",
            "iteration 668 Loss 0.28839908949181353\n",
            "iteration 669 Loss 0.2883982988909663\n",
            "iteration 670 Loss 0.2883975082972674\n",
            "iteration 671 Loss 0.2883967177107167\n",
            "iteration 672 Loss 0.28839592713131385\n",
            "iteration 673 Loss 0.2883951365590586\n",
            "iteration 674 Loss 0.28839434599395075\n",
            "iteration 675 Loss 0.28839355543599016\n",
            "iteration 676 Loss 0.28839276488517657\n",
            "iteration 677 Loss 0.2883919743415098\n",
            "iteration 678 Loss 0.2883911838049897\n",
            "iteration 679 Loss 0.2883903932756159\n",
            "iteration 680 Loss 0.2883896027533884\n",
            "iteration 681 Loss 0.2883888122383069\n",
            "iteration 682 Loss 0.2883880217303713\n",
            "iteration 683 Loss 0.2883872312295814\n",
            "iteration 684 Loss 0.28838644073593694\n",
            "iteration 685 Loss 0.2883856502494379\n",
            "iteration 686 Loss 0.28838485977008393\n",
            "iteration 687 Loss 0.288384069297875\n",
            "iteration 688 Loss 0.28838327883281095\n",
            "iteration 689 Loss 0.28838248837489155\n",
            "iteration 690 Loss 0.28838169792411666\n",
            "iteration 691 Loss 0.2883809074804861\n",
            "iteration 692 Loss 0.28838011704399985\n",
            "iteration 693 Loss 0.2883793266146576\n",
            "iteration 694 Loss 0.2883785361924593\n",
            "iteration 695 Loss 0.28837774577740477\n",
            "iteration 696 Loss 0.28837695536949387\n",
            "iteration 697 Loss 0.2883761649687265\n",
            "iteration 698 Loss 0.2883753745751025\n",
            "iteration 699 Loss 0.2883745841886216\n",
            "iteration 700 Loss 0.288373793809284\n",
            "iteration 701 Loss 0.28837300343708927\n",
            "iteration 702 Loss 0.28837221307203736\n",
            "iteration 703 Loss 0.28837142271412824\n",
            "iteration 704 Loss 0.28837063236336163\n",
            "iteration 705 Loss 0.28836984201973753\n",
            "iteration 706 Loss 0.2883690516832558\n",
            "iteration 707 Loss 0.2883682613539162\n",
            "iteration 708 Loss 0.28836747103171884\n",
            "iteration 709 Loss 0.2883666807166634\n",
            "iteration 710 Loss 0.2883658904087499\n",
            "iteration 711 Loss 0.2883651001079781\n",
            "iteration 712 Loss 0.288364309814348\n",
            "iteration 713 Loss 0.28836351952785944\n",
            "iteration 714 Loss 0.28836272924851225\n",
            "iteration 715 Loss 0.2883619389763065\n",
            "iteration 716 Loss 0.288361148711242\n",
            "iteration 717 Loss 0.28836035845331853\n",
            "iteration 718 Loss 0.2883595682025361\n",
            "iteration 719 Loss 0.2883587779588947\n",
            "iteration 720 Loss 0.28835798772239396\n",
            "iteration 721 Loss 0.288357197493034\n",
            "iteration 722 Loss 0.28835640727081463\n",
            "iteration 723 Loss 0.28835561705573587\n",
            "iteration 724 Loss 0.28835482684779745\n",
            "iteration 725 Loss 0.2883540366469994\n",
            "iteration 726 Loss 0.28835324645334154\n",
            "iteration 727 Loss 0.2883524562668239\n",
            "iteration 728 Loss 0.28835166608744617\n",
            "iteration 729 Loss 0.2883508759152085\n",
            "iteration 730 Loss 0.28835008575011073\n",
            "iteration 731 Loss 0.2883492955921527\n",
            "iteration 732 Loss 0.28834850544133434\n",
            "iteration 733 Loss 0.2883477152976555\n",
            "iteration 734 Loss 0.2883469251611163\n",
            "iteration 735 Loss 0.28834613503171647\n",
            "iteration 736 Loss 0.28834534490945596\n",
            "iteration 737 Loss 0.28834455479433474\n",
            "iteration 738 Loss 0.28834376468635264\n",
            "iteration 739 Loss 0.2883429745855096\n",
            "iteration 740 Loss 0.2883421844918056\n",
            "iteration 741 Loss 0.2883413944052404\n",
            "iteration 742 Loss 0.28834060432581415\n",
            "iteration 743 Loss 0.2883398142535266\n",
            "iteration 744 Loss 0.2883390241883777\n",
            "iteration 745 Loss 0.28833823413036747\n",
            "iteration 746 Loss 0.28833744407949563\n",
            "iteration 747 Loss 0.2883366540357622\n",
            "iteration 748 Loss 0.28833586399916716\n",
            "iteration 749 Loss 0.2883350739697103\n",
            "iteration 750 Loss 0.2883342839473917\n",
            "iteration 751 Loss 0.2883334939322112\n",
            "iteration 752 Loss 0.2883327039241687\n",
            "iteration 753 Loss 0.2883319139232642\n",
            "iteration 754 Loss 0.28833112392949745\n",
            "iteration 755 Loss 0.2883303339428686\n",
            "iteration 756 Loss 0.28832954396337734\n",
            "iteration 757 Loss 0.28832875399102376\n",
            "iteration 758 Loss 0.28832796402580785\n",
            "iteration 759 Loss 0.28832717406772934\n",
            "iteration 760 Loss 0.28832638411678824\n",
            "iteration 761 Loss 0.2883255941729845\n",
            "iteration 762 Loss 0.28832480423631796\n",
            "iteration 763 Loss 0.28832401430678867\n",
            "iteration 764 Loss 0.28832322438439645\n",
            "iteration 765 Loss 0.28832243446914124\n",
            "iteration 766 Loss 0.28832164456102305\n",
            "iteration 767 Loss 0.28832085466004176\n",
            "iteration 768 Loss 0.2883200647661973\n",
            "iteration 769 Loss 0.28831927487948955\n",
            "iteration 770 Loss 0.28831848499991847\n",
            "iteration 771 Loss 0.28831769512748395\n",
            "iteration 772 Loss 0.28831690526218606\n",
            "iteration 773 Loss 0.2883161154040245\n",
            "iteration 774 Loss 0.2883153255529994\n",
            "iteration 775 Loss 0.28831453570911064\n",
            "iteration 776 Loss 0.2883137458723581\n",
            "iteration 777 Loss 0.2883129560427417\n",
            "iteration 778 Loss 0.2883121662202614\n",
            "iteration 779 Loss 0.28831137640491705\n",
            "iteration 780 Loss 0.2883105865967088\n",
            "iteration 781 Loss 0.28830979679563634\n",
            "iteration 782 Loss 0.28830900700169976\n",
            "iteration 783 Loss 0.2883082172148989\n",
            "iteration 784 Loss 0.2883074274352337\n",
            "iteration 785 Loss 0.2883066376627041\n",
            "iteration 786 Loss 0.2883058478973101\n",
            "iteration 787 Loss 0.2883050581390515\n",
            "iteration 788 Loss 0.28830426838792833\n",
            "iteration 789 Loss 0.28830347864394046\n",
            "iteration 790 Loss 0.2883026889070878\n",
            "iteration 791 Loss 0.2883018991773705\n",
            "iteration 792 Loss 0.2883011094547882\n",
            "iteration 793 Loss 0.28830031973934095\n",
            "iteration 794 Loss 0.2882995300310287\n",
            "iteration 795 Loss 0.28829874032985137\n",
            "iteration 796 Loss 0.28829795063580893\n",
            "iteration 797 Loss 0.2882971609489013\n",
            "iteration 798 Loss 0.2882963712691282\n",
            "iteration 799 Loss 0.28829558159648994\n",
            "iteration 800 Loss 0.2882947919309861\n",
            "iteration 801 Loss 0.28829400227261687\n",
            "iteration 802 Loss 0.288293212621382\n",
            "iteration 803 Loss 0.2882924229772816\n",
            "iteration 804 Loss 0.28829163334031543\n",
            "iteration 805 Loss 0.2882908437104835\n",
            "iteration 806 Loss 0.28829005408778574\n",
            "iteration 807 Loss 0.2882892644722221\n",
            "iteration 808 Loss 0.2882884748637925\n",
            "iteration 809 Loss 0.2882876852624969\n",
            "iteration 810 Loss 0.2882868956683351\n",
            "iteration 811 Loss 0.2882861060813072\n",
            "iteration 812 Loss 0.288285316501413\n",
            "iteration 813 Loss 0.28828452692865253\n",
            "iteration 814 Loss 0.2882837373630257\n",
            "iteration 815 Loss 0.28828294780453245\n",
            "iteration 816 Loss 0.28828215825317266\n",
            "iteration 817 Loss 0.28828136870894633\n",
            "iteration 818 Loss 0.2882805791718534\n",
            "iteration 819 Loss 0.2882797896418936\n",
            "iteration 820 Loss 0.28827900011906726\n",
            "iteration 821 Loss 0.28827821060337394\n",
            "iteration 822 Loss 0.28827742109481375\n",
            "iteration 823 Loss 0.28827663159338657\n",
            "iteration 824 Loss 0.28827584209909235\n",
            "iteration 825 Loss 0.28827505261193104\n",
            "iteration 826 Loss 0.2882742631319025\n",
            "iteration 827 Loss 0.28827347365900685\n",
            "iteration 828 Loss 0.2882726841932438\n",
            "iteration 829 Loss 0.2882718947346134\n",
            "iteration 830 Loss 0.2882711052831156\n",
            "iteration 831 Loss 0.28827031583875024\n",
            "iteration 832 Loss 0.2882695264015174\n",
            "iteration 833 Loss 0.28826873697141686\n",
            "iteration 834 Loss 0.2882679475484486\n",
            "iteration 835 Loss 0.2882671581326126\n",
            "iteration 836 Loss 0.2882663687239088\n",
            "iteration 837 Loss 0.2882655793223371\n",
            "iteration 838 Loss 0.28826478992789734\n",
            "iteration 839 Loss 0.28826400054058965\n",
            "iteration 840 Loss 0.28826321116041387\n",
            "iteration 841 Loss 0.2882624217873699\n",
            "iteration 842 Loss 0.2882616324214577\n",
            "iteration 843 Loss 0.28826084306267713\n",
            "iteration 844 Loss 0.28826005371102825\n",
            "iteration 845 Loss 0.28825926436651106\n",
            "iteration 846 Loss 0.2882584750291252\n",
            "iteration 847 Loss 0.2882576856988709\n",
            "iteration 848 Loss 0.2882568963757479\n",
            "iteration 849 Loss 0.28825610705975624\n",
            "iteration 850 Loss 0.28825531775089586\n",
            "iteration 851 Loss 0.2882545284491666\n",
            "iteration 852 Loss 0.2882537391545685\n",
            "iteration 853 Loss 0.2882529498671015\n",
            "iteration 854 Loss 0.28825216058676534\n",
            "iteration 855 Loss 0.28825137131356027\n",
            "iteration 856 Loss 0.28825058204748594\n",
            "iteration 857 Loss 0.2882497927885424\n",
            "iteration 858 Loss 0.28824900353672955\n",
            "iteration 859 Loss 0.28824821429204744\n",
            "iteration 860 Loss 0.2882474250544959\n",
            "iteration 861 Loss 0.28824663582407484\n",
            "iteration 862 Loss 0.2882458466007843\n",
            "iteration 863 Loss 0.28824505738462414\n",
            "iteration 864 Loss 0.28824426817559423\n",
            "iteration 865 Loss 0.2882434789736946\n",
            "iteration 866 Loss 0.28824268977892525\n",
            "iteration 867 Loss 0.2882419005912859\n",
            "iteration 868 Loss 0.28824111141077674\n",
            "iteration 869 Loss 0.28824032223739754\n",
            "iteration 870 Loss 0.2882395330711482\n",
            "iteration 871 Loss 0.2882387439120288\n",
            "iteration 872 Loss 0.2882379547600392\n",
            "iteration 873 Loss 0.2882371656151794\n",
            "iteration 874 Loss 0.28823637647744915\n",
            "iteration 875 Loss 0.28823558734684857\n",
            "iteration 876 Loss 0.28823479822337755\n",
            "iteration 877 Loss 0.28823400910703595\n",
            "iteration 878 Loss 0.2882332199978238\n",
            "iteration 879 Loss 0.288232430895741\n",
            "iteration 880 Loss 0.2882316418007875\n",
            "iteration 881 Loss 0.28823085271296317\n",
            "iteration 882 Loss 0.288230063632268\n",
            "iteration 883 Loss 0.2882292745587019\n",
            "iteration 884 Loss 0.2882284854922649\n",
            "iteration 885 Loss 0.28822769643295676\n",
            "iteration 886 Loss 0.2882269073807775\n",
            "iteration 887 Loss 0.28822611833572714\n",
            "iteration 888 Loss 0.2882253292978054\n",
            "iteration 889 Loss 0.2882245402670125\n",
            "iteration 890 Loss 0.28822375124334826\n",
            "iteration 891 Loss 0.2882229622268125\n",
            "iteration 892 Loss 0.2882221732174052\n",
            "iteration 893 Loss 0.2882213842151264\n",
            "iteration 894 Loss 0.2882205952199759\n",
            "iteration 895 Loss 0.28821980623195376\n",
            "iteration 896 Loss 0.28821901725105986\n",
            "iteration 897 Loss 0.28821822827729404\n",
            "iteration 898 Loss 0.28821743931065635\n",
            "iteration 899 Loss 0.28821665035114674\n",
            "iteration 900 Loss 0.28821586139876515\n",
            "iteration 901 Loss 0.2882150724535114\n",
            "iteration 902 Loss 0.2882142835153855\n",
            "iteration 903 Loss 0.2882134945843874\n",
            "iteration 904 Loss 0.288212705660517\n",
            "iteration 905 Loss 0.2882119167437742\n",
            "iteration 906 Loss 0.2882111278341591\n",
            "iteration 907 Loss 0.2882103389316715\n",
            "iteration 908 Loss 0.28820955003631127\n",
            "iteration 909 Loss 0.2882087611480784\n",
            "iteration 910 Loss 0.288207972266973\n",
            "iteration 911 Loss 0.2882071833929947\n",
            "iteration 912 Loss 0.2882063945261437\n",
            "iteration 913 Loss 0.28820560566641984\n",
            "iteration 914 Loss 0.28820481681382293\n",
            "iteration 915 Loss 0.2882040279683531\n",
            "iteration 916 Loss 0.2882032391300102\n",
            "iteration 917 Loss 0.28820245029879415\n",
            "iteration 918 Loss 0.2882016614747049\n",
            "iteration 919 Loss 0.2882008726577423\n",
            "iteration 920 Loss 0.2882000838479065\n",
            "iteration 921 Loss 0.28819929504519726\n",
            "iteration 922 Loss 0.28819850624961457\n",
            "iteration 923 Loss 0.2881977174611584\n",
            "iteration 924 Loss 0.28819692867982855\n",
            "iteration 925 Loss 0.2881961399056251\n",
            "iteration 926 Loss 0.2881953511385479\n",
            "iteration 927 Loss 0.288194562378597\n",
            "iteration 928 Loss 0.28819377362577214\n",
            "iteration 929 Loss 0.28819298488007333\n",
            "iteration 930 Loss 0.28819219614150066\n",
            "iteration 931 Loss 0.28819140741005383\n",
            "iteration 932 Loss 0.288190618685733\n",
            "iteration 933 Loss 0.2881898299685379\n",
            "iteration 934 Loss 0.2881890412584686\n",
            "iteration 935 Loss 0.288188252555525\n",
            "iteration 936 Loss 0.288187463859707\n",
            "iteration 937 Loss 0.28818667517101465\n",
            "iteration 938 Loss 0.2881858864894477\n",
            "iteration 939 Loss 0.28818509781500623\n",
            "iteration 940 Loss 0.2881843091476901\n",
            "iteration 941 Loss 0.28818352048749935\n",
            "iteration 942 Loss 0.2881827318344337\n",
            "iteration 943 Loss 0.28818194318849333\n",
            "iteration 944 Loss 0.288181154549678\n",
            "iteration 945 Loss 0.28818036591798774\n",
            "iteration 946 Loss 0.2881795772934225\n",
            "iteration 947 Loss 0.2881787886759821\n",
            "iteration 948 Loss 0.2881780000656666\n",
            "iteration 949 Loss 0.2881772114624758\n",
            "iteration 950 Loss 0.2881764228664098\n",
            "iteration 951 Loss 0.2881756342774685\n",
            "iteration 952 Loss 0.2881748456956517\n",
            "iteration 953 Loss 0.2881740571209594\n",
            "iteration 954 Loss 0.28817326855339165\n",
            "iteration 955 Loss 0.2881724799929482\n",
            "iteration 956 Loss 0.2881716914396291\n",
            "iteration 957 Loss 0.2881709028934343\n",
            "iteration 958 Loss 0.2881701143543636\n",
            "iteration 959 Loss 0.2881693258224171\n",
            "iteration 960 Loss 0.28816853729759473\n",
            "iteration 961 Loss 0.2881677487798963\n",
            "iteration 962 Loss 0.28816696026932176\n",
            "iteration 963 Loss 0.28816617176587117\n",
            "iteration 964 Loss 0.28816538326954433\n",
            "iteration 965 Loss 0.28816459478034123\n",
            "iteration 966 Loss 0.2881638062982618\n",
            "iteration 967 Loss 0.28816301782330594\n",
            "iteration 968 Loss 0.2881622293554737\n",
            "iteration 969 Loss 0.2881614408947649\n",
            "iteration 970 Loss 0.28816065244117955\n",
            "iteration 971 Loss 0.28815986399471755\n",
            "iteration 972 Loss 0.28815907555537884\n",
            "iteration 973 Loss 0.28815828712316327\n",
            "iteration 974 Loss 0.28815749869807095\n",
            "iteration 975 Loss 0.28815671028010165\n",
            "iteration 976 Loss 0.28815592186925537\n",
            "iteration 977 Loss 0.2881551334655321\n",
            "iteration 978 Loss 0.28815434506893167\n",
            "iteration 979 Loss 0.28815355667945414\n",
            "iteration 980 Loss 0.2881527682970993\n",
            "iteration 981 Loss 0.2881519799218672\n",
            "iteration 982 Loss 0.28815119155375774\n",
            "iteration 983 Loss 0.2881504031927708\n",
            "iteration 984 Loss 0.28814961483890644\n",
            "iteration 985 Loss 0.2881488264921645\n",
            "iteration 986 Loss 0.28814803815254486\n",
            "iteration 987 Loss 0.2881472498200476\n",
            "iteration 988 Loss 0.2881464614946725\n",
            "iteration 989 Loss 0.28814567317641965\n",
            "iteration 990 Loss 0.2881448848652889\n",
            "iteration 991 Loss 0.2881440965612802\n",
            "iteration 992 Loss 0.28814330826439344\n",
            "iteration 993 Loss 0.28814251997462864\n",
            "iteration 994 Loss 0.28814173169198565\n",
            "iteration 995 Loss 0.2881409434164645\n",
            "iteration 996 Loss 0.28814015514806507\n",
            "iteration 997 Loss 0.28813936688678726\n",
            "iteration 998 Loss 0.2881385786326311\n",
            "iteration 999 Loss 0.2881377903855964\n",
            "iteration 1000 Loss 0.28813700214568316\n",
            "iteration 1001 Loss 0.2881362139128913\n",
            "iteration 1002 Loss 0.2881354256872209\n",
            "iteration 1003 Loss 0.2881346374686717\n",
            "iteration 1004 Loss 0.2881338492572436\n",
            "iteration 1005 Loss 0.2881330610529367\n",
            "iteration 1006 Loss 0.2881322728557509\n",
            "iteration 1007 Loss 0.2881314846656861\n",
            "iteration 1008 Loss 0.28813069648274225\n",
            "iteration 1009 Loss 0.28812990830691926\n",
            "iteration 1010 Loss 0.28812912013821707\n",
            "iteration 1011 Loss 0.2881283319766356\n",
            "iteration 1012 Loss 0.28812754382217487\n",
            "iteration 1013 Loss 0.2881267556748347\n",
            "iteration 1014 Loss 0.28812596753461517\n",
            "iteration 1015 Loss 0.288125179401516\n",
            "iteration 1016 Loss 0.28812439127553735\n",
            "iteration 1017 Loss 0.28812360315667895\n",
            "iteration 1018 Loss 0.28812281504494097\n",
            "iteration 1019 Loss 0.28812202694032313\n",
            "iteration 1020 Loss 0.2881212388428255\n",
            "iteration 1021 Loss 0.2881204507524479\n",
            "iteration 1022 Loss 0.28811966266919037\n",
            "iteration 1023 Loss 0.2881188745930528\n",
            "iteration 1024 Loss 0.2881180865240351\n",
            "iteration 1025 Loss 0.28811729846213735\n",
            "iteration 1026 Loss 0.28811651040735925\n",
            "iteration 1027 Loss 0.2881157223597009\n",
            "iteration 1028 Loss 0.2881149343191622\n",
            "iteration 1029 Loss 0.288114146285743\n",
            "iteration 1030 Loss 0.2881133582594434\n",
            "iteration 1031 Loss 0.2881125702402631\n",
            "iteration 1032 Loss 0.28811178222820233\n",
            "iteration 1033 Loss 0.28811099422326086\n",
            "iteration 1034 Loss 0.2881102062254386\n",
            "iteration 1035 Loss 0.2881094182347355\n",
            "iteration 1036 Loss 0.2881086302511516\n",
            "iteration 1037 Loss 0.28810784227468667\n",
            "iteration 1038 Loss 0.2881070543053407\n",
            "iteration 1039 Loss 0.28810626634311376\n",
            "iteration 1040 Loss 0.2881054783880056\n",
            "iteration 1041 Loss 0.2881046904400163\n",
            "iteration 1042 Loss 0.28810390249914564\n",
            "iteration 1043 Loss 0.2881031145653937\n",
            "iteration 1044 Loss 0.28810232663876034\n",
            "iteration 1045 Loss 0.28810153871924554\n",
            "iteration 1046 Loss 0.28810075080684916\n",
            "iteration 1047 Loss 0.28809996290157125\n",
            "iteration 1048 Loss 0.28809917500341164\n",
            "iteration 1049 Loss 0.2880983871123703\n",
            "iteration 1050 Loss 0.28809759922844713\n",
            "iteration 1051 Loss 0.28809681135164217\n",
            "iteration 1052 Loss 0.28809602348195534\n",
            "iteration 1053 Loss 0.2880952356193864\n",
            "iteration 1054 Loss 0.28809444776393545\n",
            "iteration 1055 Loss 0.2880936599156024\n",
            "iteration 1056 Loss 0.2880928720743871\n",
            "iteration 1057 Loss 0.2880920842402896\n",
            "iteration 1058 Loss 0.2880912964133098\n",
            "iteration 1059 Loss 0.2880905085934476\n",
            "iteration 1060 Loss 0.28808972078070294\n",
            "iteration 1061 Loss 0.2880889329750758\n",
            "iteration 1062 Loss 0.2880881451765661\n",
            "iteration 1063 Loss 0.2880873573851738\n",
            "iteration 1064 Loss 0.2880865696008987\n",
            "iteration 1065 Loss 0.28808578182374095\n",
            "iteration 1066 Loss 0.2880849940537002\n",
            "iteration 1067 Loss 0.2880842062907767\n",
            "iteration 1068 Loss 0.2880834185349701\n",
            "iteration 1069 Loss 0.28808263078628055\n",
            "iteration 1070 Loss 0.28808184304470796\n",
            "iteration 1071 Loss 0.28808105531025213\n",
            "iteration 1072 Loss 0.2880802675829131\n",
            "iteration 1073 Loss 0.28807947986269083\n",
            "iteration 1074 Loss 0.28807869214958515\n",
            "iteration 1075 Loss 0.28807790444359604\n",
            "iteration 1076 Loss 0.2880771167447234\n",
            "iteration 1077 Loss 0.2880763290529673\n",
            "iteration 1078 Loss 0.2880755413683276\n",
            "iteration 1079 Loss 0.28807475369080415\n",
            "iteration 1080 Loss 0.288073966020397\n",
            "iteration 1081 Loss 0.28807317835710605\n",
            "iteration 1082 Loss 0.2880723907009312\n",
            "iteration 1083 Loss 0.2880716030518724\n",
            "iteration 1084 Loss 0.2880708154099296\n",
            "iteration 1085 Loss 0.28807002777510277\n",
            "iteration 1086 Loss 0.28806924014739177\n",
            "iteration 1087 Loss 0.2880684525267966\n",
            "iteration 1088 Loss 0.2880676649133171\n",
            "iteration 1089 Loss 0.2880668773069533\n",
            "iteration 1090 Loss 0.28806608970770514\n",
            "iteration 1091 Loss 0.2880653021155725\n",
            "iteration 1092 Loss 0.2880645145305553\n",
            "iteration 1093 Loss 0.28806372695265353\n",
            "iteration 1094 Loss 0.2880629393818671\n",
            "iteration 1095 Loss 0.288062151818196\n",
            "iteration 1096 Loss 0.2880613642616401\n",
            "iteration 1097 Loss 0.28806057671219926\n",
            "iteration 1098 Loss 0.2880597891698736\n",
            "iteration 1099 Loss 0.28805900163466297\n",
            "iteration 1100 Loss 0.28805821410656723\n",
            "iteration 1101 Loss 0.28805742658558636\n",
            "iteration 1102 Loss 0.28805663907172047\n",
            "iteration 1103 Loss 0.28805585156496916\n",
            "iteration 1104 Loss 0.28805506406533266\n",
            "iteration 1105 Loss 0.2880542765728107\n",
            "iteration 1106 Loss 0.2880534890874033\n",
            "iteration 1107 Loss 0.2880527016091105\n",
            "iteration 1108 Loss 0.288051914137932\n",
            "iteration 1109 Loss 0.288051126673868\n",
            "iteration 1110 Loss 0.28805033921691825\n",
            "iteration 1111 Loss 0.2880495517670828\n",
            "iteration 1112 Loss 0.28804876432436144\n",
            "iteration 1113 Loss 0.2880479768887542\n",
            "iteration 1114 Loss 0.28804718946026103\n",
            "iteration 1115 Loss 0.28804640203888177\n",
            "iteration 1116 Loss 0.28804561462461653\n",
            "iteration 1117 Loss 0.28804482721746505\n",
            "iteration 1118 Loss 0.2880440398174274\n",
            "iteration 1119 Loss 0.2880432524245035\n",
            "iteration 1120 Loss 0.2880424650386932\n",
            "iteration 1121 Loss 0.2880416776599965\n",
            "iteration 1122 Loss 0.2880408902884133\n",
            "iteration 1123 Loss 0.2880401029239436\n",
            "iteration 1124 Loss 0.28803931556658724\n",
            "iteration 1125 Loss 0.28803852821634424\n",
            "iteration 1126 Loss 0.28803774087321454\n",
            "iteration 1127 Loss 0.288036953537198\n",
            "iteration 1128 Loss 0.2880361662082946\n",
            "iteration 1129 Loss 0.28803537888650427\n",
            "iteration 1130 Loss 0.2880345915718269\n",
            "iteration 1131 Loss 0.28803380426426256\n",
            "iteration 1132 Loss 0.288033016963811\n",
            "iteration 1133 Loss 0.28803222967047226\n",
            "iteration 1134 Loss 0.28803144238424633\n",
            "iteration 1135 Loss 0.288030655105133\n",
            "iteration 1136 Loss 0.28802986783313234\n",
            "iteration 1137 Loss 0.2880290805682442\n",
            "iteration 1138 Loss 0.28802829331046853\n",
            "iteration 1139 Loss 0.28802750605980537\n",
            "iteration 1140 Loss 0.28802671881625447\n",
            "iteration 1141 Loss 0.2880259315798159\n",
            "iteration 1142 Loss 0.2880251443504895\n",
            "iteration 1143 Loss 0.2880243571282753\n",
            "iteration 1144 Loss 0.2880235699131731\n",
            "iteration 1145 Loss 0.28802278270518306\n",
            "iteration 1146 Loss 0.28802199550430496\n",
            "iteration 1147 Loss 0.2880212083105387\n",
            "iteration 1148 Loss 0.28802042112388426\n",
            "iteration 1149 Loss 0.28801963394434166\n",
            "iteration 1150 Loss 0.28801884677191075\n",
            "iteration 1151 Loss 0.28801805960659144\n",
            "iteration 1152 Loss 0.28801727244838377\n",
            "iteration 1153 Loss 0.28801648529728746\n",
            "iteration 1154 Loss 0.28801569815330275\n",
            "iteration 1155 Loss 0.28801491101642934\n",
            "iteration 1156 Loss 0.28801412388666725\n",
            "iteration 1157 Loss 0.28801333676401636\n",
            "iteration 1158 Loss 0.2880125496484767\n",
            "iteration 1159 Loss 0.2880117625400482\n",
            "iteration 1160 Loss 0.28801097543873067\n",
            "iteration 1161 Loss 0.2880101883445242\n",
            "iteration 1162 Loss 0.2880094012574286\n",
            "iteration 1163 Loss 0.2880086141774439\n",
            "iteration 1164 Loss 0.2880078271045699\n",
            "iteration 1165 Loss 0.2880070400388068\n",
            "iteration 1166 Loss 0.2880062529801542\n",
            "iteration 1167 Loss 0.2880054659286122\n",
            "iteration 1168 Loss 0.2880046788841808\n",
            "iteration 1169 Loss 0.28800389184685987\n",
            "iteration 1170 Loss 0.2880031048166493\n",
            "iteration 1171 Loss 0.28800231779354907\n",
            "iteration 1172 Loss 0.2880015307775592\n",
            "iteration 1173 Loss 0.2880007437686794\n",
            "iteration 1174 Loss 0.2879999567669098\n",
            "iteration 1175 Loss 0.2879991697722503\n",
            "iteration 1176 Loss 0.2879983827847008\n",
            "iteration 1177 Loss 0.28799759580426126\n",
            "iteration 1178 Loss 0.28799680883093154\n",
            "iteration 1179 Loss 0.2879960218647117\n",
            "iteration 1180 Loss 0.2879952349056016\n",
            "iteration 1181 Loss 0.28799444795360113\n",
            "iteration 1182 Loss 0.2879936610087103\n",
            "iteration 1183 Loss 0.2879928740709291\n",
            "iteration 1184 Loss 0.28799208714025726\n",
            "iteration 1185 Loss 0.28799130021669495\n",
            "iteration 1186 Loss 0.287990513300242\n",
            "iteration 1187 Loss 0.2879897263908983\n",
            "iteration 1188 Loss 0.28798893948866383\n",
            "iteration 1189 Loss 0.28798815259353855\n",
            "iteration 1190 Loss 0.2879873657055224\n",
            "iteration 1191 Loss 0.28798657882461526\n",
            "iteration 1192 Loss 0.287985791950817\n",
            "iteration 1193 Loss 0.28798500508412783\n",
            "iteration 1194 Loss 0.2879842182245474\n",
            "iteration 1195 Loss 0.28798343137207566\n",
            "iteration 1196 Loss 0.2879826445267128\n",
            "iteration 1197 Loss 0.28798185768845846\n",
            "iteration 1198 Loss 0.2879810708573127\n",
            "iteration 1199 Loss 0.2879802840332756\n",
            "iteration 1200 Loss 0.2879794972163468\n",
            "iteration 1201 Loss 0.28797871040652645\n",
            "iteration 1202 Loss 0.2879779236038144\n",
            "iteration 1203 Loss 0.28797713680821063\n",
            "iteration 1204 Loss 0.287976350019715\n",
            "iteration 1205 Loss 0.2879755632383275\n",
            "iteration 1206 Loss 0.28797477646404807\n",
            "iteration 1207 Loss 0.28797398969687665\n",
            "iteration 1208 Loss 0.2879732029368133\n",
            "iteration 1209 Loss 0.2879724161838576\n",
            "iteration 1210 Loss 0.28797162943800975\n",
            "iteration 1211 Loss 0.28797084269926965\n",
            "iteration 1212 Loss 0.2879700559676372\n",
            "iteration 1213 Loss 0.2879692692431124\n",
            "iteration 1214 Loss 0.2879684825256951\n",
            "iteration 1215 Loss 0.28796769581538534\n",
            "iteration 1216 Loss 0.2879669091121829\n",
            "iteration 1217 Loss 0.28796612241608793\n",
            "iteration 1218 Loss 0.28796533572710015\n",
            "iteration 1219 Loss 0.28796454904521956\n",
            "iteration 1220 Loss 0.28796376237044624\n",
            "iteration 1221 Loss 0.2879629757027799\n",
            "iteration 1222 Loss 0.2879621890422206\n",
            "iteration 1223 Loss 0.2879614023887682\n",
            "iteration 1224 Loss 0.2879606157424228\n",
            "iteration 1225 Loss 0.2879598291031842\n",
            "iteration 1226 Loss 0.28795904247105225\n",
            "iteration 1227 Loss 0.28795825584602713\n",
            "iteration 1228 Loss 0.2879574692281086\n",
            "iteration 1229 Loss 0.2879566826172966\n",
            "iteration 1230 Loss 0.2879558960135911\n",
            "iteration 1231 Loss 0.2879551094169921\n",
            "iteration 1232 Loss 0.28795432282749944\n",
            "iteration 1233 Loss 0.28795353624511305\n",
            "iteration 1234 Loss 0.2879527496698329\n",
            "iteration 1235 Loss 0.28795196310165894\n",
            "iteration 1236 Loss 0.2879511765405911\n",
            "iteration 1237 Loss 0.2879503899866293\n",
            "iteration 1238 Loss 0.2879496034397735\n",
            "iteration 1239 Loss 0.28794881690002366\n",
            "iteration 1240 Loss 0.28794803036737954\n",
            "iteration 1241 Loss 0.2879472438418413\n",
            "iteration 1242 Loss 0.28794645732340873\n",
            "iteration 1243 Loss 0.28794567081208183\n",
            "iteration 1244 Loss 0.2879448843078605\n",
            "iteration 1245 Loss 0.28794409781074476\n",
            "iteration 1246 Loss 0.2879433113207344\n",
            "iteration 1247 Loss 0.2879425248378295\n",
            "iteration 1248 Loss 0.2879417383620299\n",
            "iteration 1249 Loss 0.2879409518933356\n",
            "iteration 1250 Loss 0.2879401654317464\n",
            "iteration 1251 Loss 0.28793937897726246\n",
            "iteration 1252 Loss 0.2879385925298835\n",
            "iteration 1253 Loss 0.28793780608960956\n",
            "iteration 1254 Loss 0.2879370196564406\n",
            "iteration 1255 Loss 0.2879362332303765\n",
            "iteration 1256 Loss 0.2879354468114172\n",
            "iteration 1257 Loss 0.2879346603995626\n",
            "iteration 1258 Loss 0.28793387399481274\n",
            "iteration 1259 Loss 0.28793308759716746\n",
            "iteration 1260 Loss 0.2879323012066268\n",
            "iteration 1261 Loss 0.2879315148231905\n",
            "iteration 1262 Loss 0.2879307284468587\n",
            "iteration 1263 Loss 0.28792994207763123\n",
            "iteration 1264 Loss 0.2879291557155081\n",
            "iteration 1265 Loss 0.2879283693604891\n",
            "iteration 1266 Loss 0.2879275830125743\n",
            "iteration 1267 Loss 0.28792679667176363\n",
            "iteration 1268 Loss 0.287926010338057\n",
            "iteration 1269 Loss 0.28792522401145426\n",
            "iteration 1270 Loss 0.2879244376919555\n",
            "iteration 1271 Loss 0.28792365137956055\n",
            "iteration 1272 Loss 0.28792286507426934\n",
            "iteration 1273 Loss 0.28792207877608184\n",
            "iteration 1274 Loss 0.287921292484998\n",
            "iteration 1275 Loss 0.2879205062010177\n",
            "iteration 1276 Loss 0.28791971992414095\n",
            "iteration 1277 Loss 0.2879189336543676\n",
            "iteration 1278 Loss 0.2879181473916977\n",
            "iteration 1279 Loss 0.2879173611361311\n",
            "iteration 1280 Loss 0.2879165748876677\n",
            "iteration 1281 Loss 0.28791578864630746\n",
            "iteration 1282 Loss 0.2879150024120504\n",
            "iteration 1283 Loss 0.2879142161848964\n",
            "iteration 1284 Loss 0.2879134299648453\n",
            "iteration 1285 Loss 0.28791264375189723\n",
            "iteration 1286 Loss 0.287911857546052\n",
            "iteration 1287 Loss 0.28791107134730953\n",
            "iteration 1288 Loss 0.2879102851556698\n",
            "iteration 1289 Loss 0.2879094989711327\n",
            "iteration 1290 Loss 0.2879087127936983\n",
            "iteration 1291 Loss 0.28790792662336634\n",
            "iteration 1292 Loss 0.2879071404601369\n",
            "iteration 1293 Loss 0.28790635430400985\n",
            "iteration 1294 Loss 0.28790556815498514\n",
            "iteration 1295 Loss 0.28790478201306263\n",
            "iteration 1296 Loss 0.2879039958782425\n",
            "iteration 1297 Loss 0.28790320975052436\n",
            "iteration 1298 Loss 0.2879024236299083\n",
            "iteration 1299 Loss 0.28790163751639436\n",
            "iteration 1300 Loss 0.2879008514099823\n",
            "iteration 1301 Loss 0.28790006531067214\n",
            "iteration 1302 Loss 0.2878992792184638\n",
            "iteration 1303 Loss 0.2878984931333572\n",
            "iteration 1304 Loss 0.2878977070553523\n",
            "iteration 1305 Loss 0.287896920984449\n",
            "iteration 1306 Loss 0.2878961349206473\n",
            "iteration 1307 Loss 0.28789534886394713\n",
            "iteration 1308 Loss 0.2878945628143483\n",
            "iteration 1309 Loss 0.2878937767718509\n",
            "iteration 1310 Loss 0.28789299073645475\n",
            "iteration 1311 Loss 0.28789220470815985\n",
            "iteration 1312 Loss 0.2878914186869661\n",
            "iteration 1313 Loss 0.2878906326728735\n",
            "iteration 1314 Loss 0.2878898466658819\n",
            "iteration 1315 Loss 0.28788906066599135\n",
            "iteration 1316 Loss 0.2878882746732016\n",
            "iteration 1317 Loss 0.2878874886875128\n",
            "iteration 1318 Loss 0.2878867027089247\n",
            "iteration 1319 Loss 0.28788591673743735\n",
            "iteration 1320 Loss 0.2878851307730506\n",
            "iteration 1321 Loss 0.2878843448157645\n",
            "iteration 1322 Loss 0.28788355886557887\n",
            "iteration 1323 Loss 0.28788277292249376\n",
            "iteration 1324 Loss 0.28788198698650896\n",
            "iteration 1325 Loss 0.28788120105762455\n",
            "iteration 1326 Loss 0.2878804151358404\n",
            "iteration 1327 Loss 0.2878796292211564\n",
            "iteration 1328 Loss 0.2878788433135725\n",
            "iteration 1329 Loss 0.2878780574130887\n",
            "iteration 1330 Loss 0.2878772715197049\n",
            "iteration 1331 Loss 0.28787648563342105\n",
            "iteration 1332 Loss 0.28787569975423705\n",
            "iteration 1333 Loss 0.28787491388215286\n",
            "iteration 1334 Loss 0.28787412801716844\n",
            "iteration 1335 Loss 0.2878733421592837\n",
            "iteration 1336 Loss 0.2878725563084985\n",
            "iteration 1337 Loss 0.28787177046481294\n",
            "iteration 1338 Loss 0.28787098462822674\n",
            "iteration 1339 Loss 0.28787019879874\n",
            "iteration 1340 Loss 0.28786941297635266\n",
            "iteration 1341 Loss 0.28786862716106454\n",
            "iteration 1342 Loss 0.2878678413528757\n",
            "iteration 1343 Loss 0.287867055551786\n",
            "iteration 1344 Loss 0.2878662697577954\n",
            "iteration 1345 Loss 0.2878654839709038\n",
            "iteration 1346 Loss 0.28786469819111116\n",
            "iteration 1347 Loss 0.2878639124184174\n",
            "iteration 1348 Loss 0.2878631266528225\n",
            "iteration 1349 Loss 0.28786234089432633\n",
            "iteration 1350 Loss 0.2878615551429289\n",
            "iteration 1351 Loss 0.2878607693986301\n",
            "iteration 1352 Loss 0.28785998366142984\n",
            "iteration 1353 Loss 0.287859197931328\n",
            "iteration 1354 Loss 0.28785841220832475\n",
            "iteration 1355 Loss 0.28785762649241986\n",
            "iteration 1356 Loss 0.28785684078361323\n",
            "iteration 1357 Loss 0.28785605508190487\n",
            "iteration 1358 Loss 0.28785526938729467\n",
            "iteration 1359 Loss 0.28785448369978256\n",
            "iteration 1360 Loss 0.28785369801936855\n",
            "iteration 1361 Loss 0.2878529123460525\n",
            "iteration 1362 Loss 0.2878521266798344\n",
            "iteration 1363 Loss 0.28785134102071414\n",
            "iteration 1364 Loss 0.28785055536869164\n",
            "iteration 1365 Loss 0.2878497697237669\n",
            "iteration 1366 Loss 0.28784898408593984\n",
            "iteration 1367 Loss 0.2878481984552103\n",
            "iteration 1368 Loss 0.2878474128315784\n",
            "iteration 1369 Loss 0.2878466272150438\n",
            "iteration 1370 Loss 0.2878458416056067\n",
            "iteration 1371 Loss 0.28784505600326693\n",
            "iteration 1372 Loss 0.28784427040802446\n",
            "iteration 1373 Loss 0.28784348481987915\n",
            "iteration 1374 Loss 0.28784269923883105\n",
            "iteration 1375 Loss 0.28784191366487993\n",
            "iteration 1376 Loss 0.28784112809802587\n",
            "iteration 1377 Loss 0.28784034253826873\n",
            "iteration 1378 Loss 0.2878395569856085\n",
            "iteration 1379 Loss 0.2878387714400451\n",
            "iteration 1380 Loss 0.2878379859015784\n",
            "iteration 1381 Loss 0.2878372003702084\n",
            "iteration 1382 Loss 0.28783641484593503\n",
            "iteration 1383 Loss 0.2878356293287582\n",
            "iteration 1384 Loss 0.2878348438186779\n",
            "iteration 1385 Loss 0.28783405831569403\n",
            "iteration 1386 Loss 0.2878332728198065\n",
            "iteration 1387 Loss 0.28783248733101524\n",
            "iteration 1388 Loss 0.2878317018493202\n",
            "iteration 1389 Loss 0.28783091637472136\n",
            "iteration 1390 Loss 0.2878301309072186\n",
            "iteration 1391 Loss 0.2878293454468119\n",
            "iteration 1392 Loss 0.2878285599935012\n",
            "iteration 1393 Loss 0.28782777454728636\n",
            "iteration 1394 Loss 0.28782698910816734\n",
            "iteration 1395 Loss 0.2878262036761441\n",
            "iteration 1396 Loss 0.2878254182512166\n",
            "iteration 1397 Loss 0.2878246328333848\n",
            "iteration 1398 Loss 0.2878238474226485\n",
            "iteration 1399 Loss 0.2878230620190077\n",
            "iteration 1400 Loss 0.28782227662246246\n",
            "iteration 1401 Loss 0.2878214912330124\n",
            "iteration 1402 Loss 0.28782070585065783\n",
            "iteration 1403 Loss 0.2878199204753985\n",
            "iteration 1404 Loss 0.28781913510723434\n",
            "iteration 1405 Loss 0.2878183497461652\n",
            "iteration 1406 Loss 0.28781756439219136\n",
            "iteration 1407 Loss 0.2878167790453124\n",
            "iteration 1408 Loss 0.2878159937055283\n",
            "iteration 1409 Loss 0.28781520837283914\n",
            "iteration 1410 Loss 0.28781442304724475\n",
            "iteration 1411 Loss 0.2878136377287452\n",
            "iteration 1412 Loss 0.2878128524173401\n",
            "iteration 1413 Loss 0.2878120671130298\n",
            "iteration 1414 Loss 0.28781128181581395\n",
            "iteration 1415 Loss 0.2878104965256926\n",
            "iteration 1416 Loss 0.28780971124266563\n",
            "iteration 1417 Loss 0.2878089259667331\n",
            "iteration 1418 Loss 0.2878081406978948\n",
            "iteration 1419 Loss 0.28780735543615066\n",
            "iteration 1420 Loss 0.28780657018150074\n",
            "iteration 1421 Loss 0.2878057849339449\n",
            "iteration 1422 Loss 0.28780499969348305\n",
            "iteration 1423 Loss 0.2878042144601152\n",
            "iteration 1424 Loss 0.28780342923384117\n",
            "iteration 1425 Loss 0.2878026440146611\n",
            "iteration 1426 Loss 0.28780185880257464\n",
            "iteration 1427 Loss 0.28780107359758195\n",
            "iteration 1428 Loss 0.2878002883996829\n",
            "iteration 1429 Loss 0.28779950320887737\n",
            "iteration 1430 Loss 0.2877987180251654\n",
            "iteration 1431 Loss 0.2877979328485468\n",
            "iteration 1432 Loss 0.2877971476790217\n",
            "iteration 1433 Loss 0.28779636251658974\n",
            "iteration 1434 Loss 0.2877955773612511\n",
            "iteration 1435 Loss 0.2877947922130057\n",
            "iteration 1436 Loss 0.2877940070718534\n",
            "iteration 1437 Loss 0.287793221937794\n",
            "iteration 1438 Loss 0.2877924368108277\n",
            "iteration 1439 Loss 0.28779165169095433\n",
            "iteration 1440 Loss 0.28779086657817377\n",
            "iteration 1441 Loss 0.28779008147248597\n",
            "iteration 1442 Loss 0.28778929637389095\n",
            "iteration 1443 Loss 0.2877885112823885\n",
            "iteration 1444 Loss 0.2877877261979788\n",
            "iteration 1445 Loss 0.2877869411206615\n",
            "iteration 1446 Loss 0.2877861560504367\n",
            "iteration 1447 Loss 0.28778537098730433\n",
            "iteration 1448 Loss 0.28778458593126427\n",
            "iteration 1449 Loss 0.2877838008823164\n",
            "iteration 1450 Loss 0.2877830158404608\n",
            "iteration 1451 Loss 0.28778223080569737\n",
            "iteration 1452 Loss 0.287781445778026\n",
            "iteration 1453 Loss 0.2877806607574466\n",
            "iteration 1454 Loss 0.2877798757439592\n",
            "iteration 1455 Loss 0.28777909073756364\n",
            "iteration 1456 Loss 0.28777830573825985\n",
            "iteration 1457 Loss 0.2877775207460479\n",
            "iteration 1458 Loss 0.2877767357609275\n",
            "iteration 1459 Loss 0.28777595078289886\n",
            "iteration 1460 Loss 0.2877751658119618\n",
            "iteration 1461 Loss 0.2877743808481161\n",
            "iteration 1462 Loss 0.2877735958913618\n",
            "iteration 1463 Loss 0.287772810941699\n",
            "iteration 1464 Loss 0.2877720259991274\n",
            "iteration 1465 Loss 0.28777124106364704\n",
            "iteration 1466 Loss 0.2877704561352578\n",
            "iteration 1467 Loss 0.28776967121395974\n",
            "iteration 1468 Loss 0.28776888629975267\n",
            "iteration 1469 Loss 0.2877681013926366\n",
            "iteration 1470 Loss 0.28776731649261145\n",
            "iteration 1471 Loss 0.2877665315996771\n",
            "iteration 1472 Loss 0.2877657467138335\n",
            "iteration 1473 Loss 0.28776496183508066\n",
            "iteration 1474 Loss 0.28776417696341844\n",
            "iteration 1475 Loss 0.2877633920988468\n",
            "iteration 1476 Loss 0.28776260724136565\n",
            "iteration 1477 Loss 0.287761822390975\n",
            "iteration 1478 Loss 0.2877610375476747\n",
            "iteration 1479 Loss 0.28776025271146466\n",
            "iteration 1480 Loss 0.287759467882345\n",
            "iteration 1481 Loss 0.28775868306031543\n",
            "iteration 1482 Loss 0.28775789824537606\n",
            "iteration 1483 Loss 0.2877571134375267\n",
            "iteration 1484 Loss 0.2877563286367674\n",
            "iteration 1485 Loss 0.287755543843098\n",
            "iteration 1486 Loss 0.2877547590565184\n",
            "iteration 1487 Loss 0.2877539742770287\n",
            "iteration 1488 Loss 0.28775318950462864\n",
            "iteration 1489 Loss 0.2877524047393183\n",
            "iteration 1490 Loss 0.2877516199810975\n",
            "iteration 1491 Loss 0.2877508352299663\n",
            "iteration 1492 Loss 0.28775005048592456\n",
            "iteration 1493 Loss 0.28774926574897225\n",
            "iteration 1494 Loss 0.28774848101910927\n",
            "iteration 1495 Loss 0.2877476962963356\n",
            "iteration 1496 Loss 0.2877469115806511\n",
            "iteration 1497 Loss 0.28774612687205575\n",
            "iteration 1498 Loss 0.28774534217054953\n",
            "iteration 1499 Loss 0.2877445574761323\n",
            "iteration 1500 Loss 0.28774377278880403\n",
            "iteration 1501 Loss 0.28774298810856463\n",
            "iteration 1502 Loss 0.2877422034354141\n",
            "iteration 1503 Loss 0.2877414187693523\n",
            "iteration 1504 Loss 0.2877406341103792\n",
            "iteration 1505 Loss 0.2877398494584947\n",
            "iteration 1506 Loss 0.28773906481369876\n",
            "iteration 1507 Loss 0.2877382801759914\n",
            "iteration 1508 Loss 0.2877374955453724\n",
            "iteration 1509 Loss 0.28773671092184183\n",
            "iteration 1510 Loss 0.2877359263053995\n",
            "iteration 1511 Loss 0.28773514169604547\n",
            "iteration 1512 Loss 0.28773435709377965\n",
            "iteration 1513 Loss 0.2877335724986019\n",
            "iteration 1514 Loss 0.28773278791051216\n",
            "iteration 1515 Loss 0.28773200332951043\n",
            "iteration 1516 Loss 0.28773121875559665\n",
            "iteration 1517 Loss 0.28773043418877065\n",
            "iteration 1518 Loss 0.28772964962903247\n",
            "iteration 1519 Loss 0.287728865076382\n",
            "iteration 1520 Loss 0.2877280805308193\n",
            "iteration 1521 Loss 0.287727295992344\n",
            "iteration 1522 Loss 0.28772651146095635\n",
            "iteration 1523 Loss 0.2877257269366561\n",
            "iteration 1524 Loss 0.2877249424194433\n",
            "iteration 1525 Loss 0.2877241579093178\n",
            "iteration 1526 Loss 0.28772337340627957\n",
            "iteration 1527 Loss 0.28772258891032854\n",
            "iteration 1528 Loss 0.2877218044214646\n",
            "iteration 1529 Loss 0.28772101993968785\n",
            "iteration 1530 Loss 0.28772023546499803\n",
            "iteration 1531 Loss 0.2877194509973952\n",
            "iteration 1532 Loss 0.28771866653687916\n",
            "iteration 1533 Loss 0.28771788208345\n",
            "iteration 1534 Loss 0.28771709763710757\n",
            "iteration 1535 Loss 0.28771631319785185\n",
            "iteration 1536 Loss 0.28771552876568274\n",
            "iteration 1537 Loss 0.2877147443406002\n",
            "iteration 1538 Loss 0.2877139599226041\n",
            "iteration 1539 Loss 0.28771317551169434\n",
            "iteration 1540 Loss 0.2877123911078711\n",
            "iteration 1541 Loss 0.2877116067111341\n",
            "iteration 1542 Loss 0.2877108223214833\n",
            "iteration 1543 Loss 0.28771003793891875\n",
            "iteration 1544 Loss 0.2877092535634402\n",
            "iteration 1545 Loss 0.28770846919504767\n",
            "iteration 1546 Loss 0.28770768483374115\n",
            "iteration 1547 Loss 0.2877069004795206\n",
            "iteration 1548 Loss 0.2877061161323858\n",
            "iteration 1549 Loss 0.28770533179233676\n",
            "iteration 1550 Loss 0.28770454745937346\n",
            "iteration 1551 Loss 0.2877037631334958\n",
            "iteration 1552 Loss 0.28770297881470375\n",
            "iteration 1553 Loss 0.28770219450299717\n",
            "iteration 1554 Loss 0.2877014101983761\n",
            "iteration 1555 Loss 0.28770062590084033\n",
            "iteration 1556 Loss 0.2876998416103899\n",
            "iteration 1557 Loss 0.2876990573270248\n",
            "iteration 1558 Loss 0.28769827305074486\n",
            "iteration 1559 Loss 0.28769748878155\n",
            "iteration 1560 Loss 0.28769670451944024\n",
            "iteration 1561 Loss 0.28769592026441543\n",
            "iteration 1562 Loss 0.28769513601647556\n",
            "iteration 1563 Loss 0.2876943517756206\n",
            "iteration 1564 Loss 0.2876935675418504\n",
            "iteration 1565 Loss 0.28769278331516496\n",
            "iteration 1566 Loss 0.28769199909556414\n",
            "iteration 1567 Loss 0.28769121488304794\n",
            "iteration 1568 Loss 0.28769043067761635\n",
            "iteration 1569 Loss 0.2876896464792692\n",
            "iteration 1570 Loss 0.28768886228800644\n",
            "iteration 1571 Loss 0.28768807810382796\n",
            "iteration 1572 Loss 0.28768729392673387\n",
            "iteration 1573 Loss 0.287686509756724\n",
            "iteration 1574 Loss 0.2876857255937982\n",
            "iteration 1575 Loss 0.2876849414379565\n",
            "iteration 1576 Loss 0.2876841572891989\n",
            "iteration 1577 Loss 0.2876833731475252\n",
            "iteration 1578 Loss 0.28768258901293536\n",
            "iteration 1579 Loss 0.28768180488542944\n",
            "iteration 1580 Loss 0.2876810207650071\n",
            "iteration 1581 Loss 0.28768023665166864\n",
            "iteration 1582 Loss 0.2876794525454137\n",
            "iteration 1583 Loss 0.2876786684462424\n",
            "iteration 1584 Loss 0.2876778843541546\n",
            "iteration 1585 Loss 0.2876771002691502\n",
            "iteration 1586 Loss 0.2876763161912292\n",
            "iteration 1587 Loss 0.28767553212039143\n",
            "iteration 1588 Loss 0.287674748056637\n",
            "iteration 1589 Loss 0.28767396399996564\n",
            "iteration 1590 Loss 0.2876731799503775\n",
            "iteration 1591 Loss 0.28767239590787236\n",
            "iteration 1592 Loss 0.2876716118724502\n",
            "iteration 1593 Loss 0.2876708278441109\n",
            "iteration 1594 Loss 0.28767004382285455\n",
            "iteration 1595 Loss 0.28766925980868097\n",
            "iteration 1596 Loss 0.2876684758015901\n",
            "iteration 1597 Loss 0.28766769180158186\n",
            "iteration 1598 Loss 0.2876669078086562\n",
            "iteration 1599 Loss 0.2876661238228132\n",
            "iteration 1600 Loss 0.2876653398440525\n",
            "iteration 1601 Loss 0.2876645558723743\n",
            "iteration 1602 Loss 0.28766377190777837\n",
            "iteration 1603 Loss 0.28766298795026474\n",
            "iteration 1604 Loss 0.2876622039998333\n",
            "iteration 1605 Loss 0.28766142005648404\n",
            "iteration 1606 Loss 0.2876606361202168\n",
            "iteration 1607 Loss 0.2876598521910316\n",
            "iteration 1608 Loss 0.28765906826892834\n",
            "iteration 1609 Loss 0.2876582843539069\n",
            "iteration 1610 Loss 0.28765750044596733\n",
            "iteration 1611 Loss 0.28765671654510955\n",
            "iteration 1612 Loss 0.28765593265133343\n",
            "iteration 1613 Loss 0.2876551487646388\n",
            "iteration 1614 Loss 0.2876543648850259\n",
            "iteration 1615 Loss 0.2876535810124944\n",
            "iteration 1616 Loss 0.2876527971470444\n",
            "iteration 1617 Loss 0.2876520132886757\n",
            "iteration 1618 Loss 0.28765122943738825\n",
            "iteration 1619 Loss 0.28765044559318204\n",
            "iteration 1620 Loss 0.28764966175605705\n",
            "iteration 1621 Loss 0.2876488779260131\n",
            "iteration 1622 Loss 0.28764809410305026\n",
            "iteration 1623 Loss 0.28764731028716833\n",
            "iteration 1624 Loss 0.28764652647836736\n",
            "iteration 1625 Loss 0.2876457426766472\n",
            "iteration 1626 Loss 0.2876449588820078\n",
            "iteration 1627 Loss 0.2876441750944491\n",
            "iteration 1628 Loss 0.2876433913139711\n",
            "iteration 1629 Loss 0.28764260754057364\n",
            "iteration 1630 Loss 0.28764182377425673\n",
            "iteration 1631 Loss 0.2876410400150203\n",
            "iteration 1632 Loss 0.2876402562628641\n",
            "iteration 1633 Loss 0.2876394725177884\n",
            "iteration 1634 Loss 0.2876386887797929\n",
            "iteration 1635 Loss 0.2876379050488775\n",
            "iteration 1636 Loss 0.2876371213250423\n",
            "iteration 1637 Loss 0.28763633760828716\n",
            "iteration 1638 Loss 0.287635553898612\n",
            "iteration 1639 Loss 0.28763477019601674\n",
            "iteration 1640 Loss 0.2876339865005014\n",
            "iteration 1641 Loss 0.2876332028120659\n",
            "iteration 1642 Loss 0.2876324191307101\n",
            "iteration 1643 Loss 0.28763163545643394\n",
            "iteration 1644 Loss 0.2876308517892374\n",
            "iteration 1645 Loss 0.28763006812912034\n",
            "iteration 1646 Loss 0.28762928447608294\n",
            "iteration 1647 Loss 0.2876285008301248\n",
            "iteration 1648 Loss 0.287627717191246\n",
            "iteration 1649 Loss 0.2876269335594466\n",
            "iteration 1650 Loss 0.28762614993472635\n",
            "iteration 1651 Loss 0.2876253663170852\n",
            "iteration 1652 Loss 0.28762458270652325\n",
            "iteration 1653 Loss 0.2876237991030402\n",
            "iteration 1654 Loss 0.28762301550663616\n",
            "iteration 1655 Loss 0.28762223191731107\n",
            "iteration 1656 Loss 0.28762144833506476\n",
            "iteration 1657 Loss 0.28762066475989717\n",
            "iteration 1658 Loss 0.28761988119180837\n",
            "iteration 1659 Loss 0.2876190976307981\n",
            "iteration 1660 Loss 0.28761831407686644\n",
            "iteration 1661 Loss 0.2876175305300134\n",
            "iteration 1662 Loss 0.2876167469902386\n",
            "iteration 1663 Loss 0.2876159634575423\n",
            "iteration 1664 Loss 0.28761517993192426\n",
            "iteration 1665 Loss 0.2876143964133845\n",
            "iteration 1666 Loss 0.2876136129019229\n",
            "iteration 1667 Loss 0.2876128293975394\n",
            "iteration 1668 Loss 0.287612045900234\n",
            "iteration 1669 Loss 0.2876112624100065\n",
            "iteration 1670 Loss 0.2876104789268569\n",
            "iteration 1671 Loss 0.2876096954507853\n",
            "iteration 1672 Loss 0.28760891198179145\n",
            "iteration 1673 Loss 0.2876081285198752\n",
            "iteration 1674 Loss 0.2876073450650367\n",
            "iteration 1675 Loss 0.2876065616172759\n",
            "iteration 1676 Loss 0.28760577817659244\n",
            "iteration 1677 Loss 0.28760499474298656\n",
            "iteration 1678 Loss 0.287604211316458\n",
            "iteration 1679 Loss 0.2876034278970069\n",
            "iteration 1680 Loss 0.28760264448463296\n",
            "iteration 1681 Loss 0.2876018610793363\n",
            "iteration 1682 Loss 0.28760107768111665\n",
            "iteration 1683 Loss 0.2876002942899742\n",
            "iteration 1684 Loss 0.2875995109059088\n",
            "iteration 1685 Loss 0.28759872752892024\n",
            "iteration 1686 Loss 0.2875979441590086\n",
            "iteration 1687 Loss 0.28759716079617376\n",
            "iteration 1688 Loss 0.28759637744041566\n",
            "iteration 1689 Loss 0.28759559409173424\n",
            "iteration 1690 Loss 0.2875948107501295\n",
            "iteration 1691 Loss 0.28759402741560125\n",
            "iteration 1692 Loss 0.28759324408814957\n",
            "iteration 1693 Loss 0.2875924607677743\n",
            "iteration 1694 Loss 0.28759167745447534\n",
            "iteration 1695 Loss 0.28759089414825273\n",
            "iteration 1696 Loss 0.2875901108491063\n",
            "iteration 1697 Loss 0.2875893275570361\n",
            "iteration 1698 Loss 0.287588544272042\n",
            "iteration 1699 Loss 0.2875877609941239\n",
            "iteration 1700 Loss 0.28758697772328173\n",
            "iteration 1701 Loss 0.28758619445951555\n",
            "iteration 1702 Loss 0.2875854112028252\n",
            "iteration 1703 Loss 0.2875846279532106\n",
            "iteration 1704 Loss 0.2875838447106717\n",
            "iteration 1705 Loss 0.2875830614752085\n",
            "iteration 1706 Loss 0.28758227824682087\n",
            "iteration 1707 Loss 0.2875814950255087\n",
            "iteration 1708 Loss 0.28758071181127204\n",
            "iteration 1709 Loss 0.28757992860411075\n",
            "iteration 1710 Loss 0.2875791454040248\n",
            "iteration 1711 Loss 0.2875783622110141\n",
            "iteration 1712 Loss 0.2875775790250787\n",
            "iteration 1713 Loss 0.28757679584621826\n",
            "iteration 1714 Loss 0.28757601267443295\n",
            "iteration 1715 Loss 0.28757522950972264\n",
            "iteration 1716 Loss 0.28757444635208723\n",
            "iteration 1717 Loss 0.2875736632015268\n",
            "iteration 1718 Loss 0.287572880058041\n",
            "iteration 1719 Loss 0.28757209692163005\n",
            "iteration 1720 Loss 0.28757131379229384\n",
            "iteration 1721 Loss 0.28757053067003213\n",
            "iteration 1722 Loss 0.287569747554845\n",
            "iteration 1723 Loss 0.28756896444673236\n",
            "iteration 1724 Loss 0.2875681813456941\n",
            "iteration 1725 Loss 0.2875673982517302\n",
            "iteration 1726 Loss 0.28756661516484067\n",
            "iteration 1727 Loss 0.2875658320850253\n",
            "iteration 1728 Loss 0.287565049012284\n",
            "iteration 1729 Loss 0.2875642659466169\n",
            "iteration 1730 Loss 0.2875634828880238\n",
            "iteration 1731 Loss 0.28756269983650473\n",
            "iteration 1732 Loss 0.28756191679205945\n",
            "iteration 1733 Loss 0.28756113375468806\n",
            "iteration 1734 Loss 0.2875603507243904\n",
            "iteration 1735 Loss 0.2875595677011665\n",
            "iteration 1736 Loss 0.2875587846850162\n",
            "iteration 1737 Loss 0.2875580016759394\n",
            "iteration 1738 Loss 0.2875572186739362\n",
            "iteration 1739 Loss 0.2875564356790064\n",
            "iteration 1740 Loss 0.28755565269115\n",
            "iteration 1741 Loss 0.2875548697103669\n",
            "iteration 1742 Loss 0.28755408673665706\n",
            "iteration 1743 Loss 0.2875533037700204\n",
            "iteration 1744 Loss 0.2875525208104569\n",
            "iteration 1745 Loss 0.2875517378579664\n",
            "iteration 1746 Loss 0.28755095491254895\n",
            "iteration 1747 Loss 0.28755017197420435\n",
            "iteration 1748 Loss 0.28754938904293265\n",
            "iteration 1749 Loss 0.28754860611873373\n",
            "iteration 1750 Loss 0.2875478232016075\n",
            "iteration 1751 Loss 0.28754704029155403\n",
            "iteration 1752 Loss 0.2875462573885731\n",
            "iteration 1753 Loss 0.2875454744926647\n",
            "iteration 1754 Loss 0.2875446916038288\n",
            "iteration 1755 Loss 0.2875439087220652\n",
            "iteration 1756 Loss 0.28754312584737407\n",
            "iteration 1757 Loss 0.28754234297975517\n",
            "iteration 1758 Loss 0.28754156011920845\n",
            "iteration 1759 Loss 0.2875407772657339\n",
            "iteration 1760 Loss 0.2875399944193315\n",
            "iteration 1761 Loss 0.287539211580001\n",
            "iteration 1762 Loss 0.28753842874774255\n",
            "iteration 1763 Loss 0.2875376459225559\n",
            "iteration 1764 Loss 0.2875368631044411\n",
            "iteration 1765 Loss 0.28753608029339806\n",
            "iteration 1766 Loss 0.2875352974894267\n",
            "iteration 1767 Loss 0.28753451469252694\n",
            "iteration 1768 Loss 0.2875337319026988\n",
            "iteration 1769 Loss 0.2875329491199422\n",
            "iteration 1770 Loss 0.28753216634425693\n",
            "iteration 1771 Loss 0.2875313835756431\n",
            "iteration 1772 Loss 0.28753060081410053\n",
            "iteration 1773 Loss 0.28752981805962924\n",
            "iteration 1774 Loss 0.287529035312229\n",
            "iteration 1775 Loss 0.2875282525719\n",
            "iteration 1776 Loss 0.287527469838642\n",
            "iteration 1777 Loss 0.287526687112455\n",
            "iteration 1778 Loss 0.28752590439333886\n",
            "iteration 1779 Loss 0.2875251216812936\n",
            "iteration 1780 Loss 0.2875243389763191\n",
            "iteration 1781 Loss 0.28752355627841536\n",
            "iteration 1782 Loss 0.2875227735875822\n",
            "iteration 1783 Loss 0.28752199090381975\n",
            "iteration 1784 Loss 0.28752120822712773\n",
            "iteration 1785 Loss 0.28752042555750623\n",
            "iteration 1786 Loss 0.2875196428949551\n",
            "iteration 1787 Loss 0.28751886023947426\n",
            "iteration 1788 Loss 0.2875180775910637\n",
            "iteration 1789 Loss 0.2875172949497234\n",
            "iteration 1790 Loss 0.2875165123154532\n",
            "iteration 1791 Loss 0.287515729688253\n",
            "iteration 1792 Loss 0.28751494706812286\n",
            "iteration 1793 Loss 0.28751416445506267\n",
            "iteration 1794 Loss 0.2875133818490723\n",
            "iteration 1795 Loss 0.2875125992501518\n",
            "iteration 1796 Loss 0.28751181665830106\n",
            "iteration 1797 Loss 0.28751103407351997\n",
            "iteration 1798 Loss 0.2875102514958085\n",
            "iteration 1799 Loss 0.28750946892516654\n",
            "iteration 1800 Loss 0.28750868636159416\n",
            "iteration 1801 Loss 0.2875079038050911\n",
            "iteration 1802 Loss 0.28750712125565747\n",
            "iteration 1803 Loss 0.2875063387132931\n",
            "iteration 1804 Loss 0.28750555617799795\n",
            "iteration 1805 Loss 0.287504773649772\n",
            "iteration 1806 Loss 0.2875039911286151\n",
            "iteration 1807 Loss 0.2875032086145272\n",
            "iteration 1808 Loss 0.2875024261075084\n",
            "iteration 1809 Loss 0.28750164360755837\n",
            "iteration 1810 Loss 0.28750086111467726\n",
            "iteration 1811 Loss 0.28750007862886484\n",
            "iteration 1812 Loss 0.2874992961501212\n",
            "iteration 1813 Loss 0.2874985136784462\n",
            "iteration 1814 Loss 0.28749773121383976\n",
            "iteration 1815 Loss 0.28749694875630183\n",
            "iteration 1816 Loss 0.28749616630583236\n",
            "iteration 1817 Loss 0.28749538386243123\n",
            "iteration 1818 Loss 0.28749460142609845\n",
            "iteration 1819 Loss 0.287493818996834\n",
            "iteration 1820 Loss 0.28749303657463765\n",
            "iteration 1821 Loss 0.28749225415950946\n",
            "iteration 1822 Loss 0.2874914717514493\n",
            "iteration 1823 Loss 0.28749068935045713\n",
            "iteration 1824 Loss 0.28748990695653287\n",
            "iteration 1825 Loss 0.2874891245696765\n",
            "iteration 1826 Loss 0.28748834218988795\n",
            "iteration 1827 Loss 0.2874875598171671\n",
            "iteration 1828 Loss 0.287486777451514\n",
            "iteration 1829 Loss 0.28748599509292844\n",
            "iteration 1830 Loss 0.2874852127414104\n",
            "iteration 1831 Loss 0.28748443039695987\n",
            "iteration 1832 Loss 0.28748364805957677\n",
            "iteration 1833 Loss 0.28748286572926096\n",
            "iteration 1834 Loss 0.2874820834060125\n",
            "iteration 1835 Loss 0.2874813010898312\n",
            "iteration 1836 Loss 0.2874805187807171\n",
            "iteration 1837 Loss 0.28747973647867003\n",
            "iteration 1838 Loss 0.28747895418369\n",
            "iteration 1839 Loss 0.28747817189577696\n",
            "iteration 1840 Loss 0.2874773896149308\n",
            "iteration 1841 Loss 0.28747660734115144\n",
            "iteration 1842 Loss 0.28747582507443886\n",
            "iteration 1843 Loss 0.28747504281479297\n",
            "iteration 1844 Loss 0.28747426056221376\n",
            "iteration 1845 Loss 0.287473478316701\n",
            "iteration 1846 Loss 0.2874726960782549\n",
            "iteration 1847 Loss 0.2874719138468751\n",
            "iteration 1848 Loss 0.28747113162256177\n",
            "iteration 1849 Loss 0.2874703494053147\n",
            "iteration 1850 Loss 0.2874695671951339\n",
            "iteration 1851 Loss 0.2874687849920192\n",
            "iteration 1852 Loss 0.28746800279597073\n",
            "iteration 1853 Loss 0.2874672206069882\n",
            "iteration 1854 Loss 0.28746643842507175\n",
            "iteration 1855 Loss 0.2874656562502212\n",
            "iteration 1856 Loss 0.2874648740824365\n",
            "iteration 1857 Loss 0.2874640919217175\n",
            "iteration 1858 Loss 0.2874633097680643\n",
            "iteration 1859 Loss 0.2874625276214768\n",
            "iteration 1860 Loss 0.28746174548195486\n",
            "iteration 1861 Loss 0.2874609633494984\n",
            "iteration 1862 Loss 0.2874601812241075\n",
            "iteration 1863 Loss 0.28745939910578194\n",
            "iteration 1864 Loss 0.2874586169945217\n",
            "iteration 1865 Loss 0.2874578348903268\n",
            "iteration 1866 Loss 0.28745705279319705\n",
            "iteration 1867 Loss 0.28745627070313245\n",
            "iteration 1868 Loss 0.2874554886201329\n",
            "iteration 1869 Loss 0.2874547065441984\n",
            "iteration 1870 Loss 0.28745392447532886\n",
            "iteration 1871 Loss 0.2874531424135242\n",
            "iteration 1872 Loss 0.28745236035878435\n",
            "iteration 1873 Loss 0.28745157831110923\n",
            "iteration 1874 Loss 0.2874507962704988\n",
            "iteration 1875 Loss 0.287450014236953\n",
            "iteration 1876 Loss 0.28744923221047175\n",
            "iteration 1877 Loss 0.287448450191055\n",
            "iteration 1878 Loss 0.28744766817870276\n",
            "iteration 1879 Loss 0.28744688617341474\n",
            "iteration 1880 Loss 0.28744610417519106\n",
            "iteration 1881 Loss 0.2874453221840317\n",
            "iteration 1882 Loss 0.2874445401999364\n",
            "iteration 1883 Loss 0.2874437582229053\n",
            "iteration 1884 Loss 0.2874429762529382\n",
            "iteration 1885 Loss 0.28744219429003504\n",
            "iteration 1886 Loss 0.2874414123341958\n",
            "iteration 1887 Loss 0.2874406303854204\n",
            "iteration 1888 Loss 0.2874398484437088\n",
            "iteration 1889 Loss 0.2874390665090609\n",
            "iteration 1890 Loss 0.28743828458147663\n",
            "iteration 1891 Loss 0.2874375026609561\n",
            "iteration 1892 Loss 0.2874367207474989\n",
            "iteration 1893 Loss 0.2874359388411052\n",
            "iteration 1894 Loss 0.28743515694177496\n",
            "iteration 1895 Loss 0.28743437504950803\n",
            "iteration 1896 Loss 0.28743359316430434\n",
            "iteration 1897 Loss 0.2874328112861638\n",
            "iteration 1898 Loss 0.28743202941508644\n",
            "iteration 1899 Loss 0.2874312475510722\n",
            "iteration 1900 Loss 0.28743046569412084\n",
            "iteration 1901 Loss 0.28742968384423245\n",
            "iteration 1902 Loss 0.28742890200140697\n",
            "iteration 1903 Loss 0.28742812016564434\n",
            "iteration 1904 Loss 0.28742733833694434\n",
            "iteration 1905 Loss 0.2874265565153071\n",
            "iteration 1906 Loss 0.28742577470073244\n",
            "iteration 1907 Loss 0.2874249928932204\n",
            "iteration 1908 Loss 0.2874242110927708\n",
            "iteration 1909 Loss 0.2874234292993835\n",
            "iteration 1910 Loss 0.28742264751305874\n",
            "iteration 1911 Loss 0.2874218657337962\n",
            "iteration 1912 Loss 0.2874210839615958\n",
            "iteration 1913 Loss 0.28742030219645764\n",
            "iteration 1914 Loss 0.28741952043838154\n",
            "iteration 1915 Loss 0.28741873868736756\n",
            "iteration 1916 Loss 0.2874179569434154\n",
            "iteration 1917 Loss 0.28741717520652527\n",
            "iteration 1918 Loss 0.2874163934766969\n",
            "iteration 1919 Loss 0.2874156117539303\n",
            "iteration 1920 Loss 0.28741483003822543\n",
            "iteration 1921 Loss 0.28741404832958223\n",
            "iteration 1922 Loss 0.2874132666280006\n",
            "iteration 1923 Loss 0.2874124849334804\n",
            "iteration 1924 Loss 0.2874117032460217\n",
            "iteration 1925 Loss 0.28741092156562437\n",
            "iteration 1926 Loss 0.2874101398922884\n",
            "iteration 1927 Loss 0.28740935822601366\n",
            "iteration 1928 Loss 0.28740857656680013\n",
            "iteration 1929 Loss 0.2874077949146477\n",
            "iteration 1930 Loss 0.28740701326955626\n",
            "iteration 1931 Loss 0.2874062316315259\n",
            "iteration 1932 Loss 0.28740545000055645\n",
            "iteration 1933 Loss 0.2874046683766479\n",
            "iteration 1934 Loss 0.28740388675980005\n",
            "iteration 1935 Loss 0.287403105150013\n",
            "iteration 1936 Loss 0.28740232354728656\n",
            "iteration 1937 Loss 0.2874015419516208\n",
            "iteration 1938 Loss 0.28740076036301554\n",
            "iteration 1939 Loss 0.2873999787814707\n",
            "iteration 1940 Loss 0.2873991972069864\n",
            "iteration 1941 Loss 0.28739841563956237\n",
            "iteration 1942 Loss 0.2873976340791986\n",
            "iteration 1943 Loss 0.2873968525258951\n",
            "iteration 1944 Loss 0.2873960709796517\n",
            "iteration 1945 Loss 0.2873952894404684\n",
            "iteration 1946 Loss 0.2873945079083451\n",
            "iteration 1947 Loss 0.28739372638328176\n",
            "iteration 1948 Loss 0.2873929448652784\n",
            "iteration 1949 Loss 0.28739216335433476\n",
            "iteration 1950 Loss 0.2873913818504509\n",
            "iteration 1951 Loss 0.28739060035362674\n",
            "iteration 1952 Loss 0.2873898188638623\n",
            "iteration 1953 Loss 0.28738903738115734\n",
            "iteration 1954 Loss 0.2873882559055118\n",
            "iteration 1955 Loss 0.28738747443692586\n",
            "iteration 1956 Loss 0.2873866929753992\n",
            "iteration 1957 Loss 0.2873859115209319\n",
            "iteration 1958 Loss 0.28738513007352384\n",
            "iteration 1959 Loss 0.2873843486331748\n",
            "iteration 1960 Loss 0.28738356719988506\n",
            "iteration 1961 Loss 0.28738278577365434\n",
            "iteration 1962 Loss 0.2873820043544825\n",
            "iteration 1963 Loss 0.28738122294236973\n",
            "iteration 1964 Loss 0.28738044153731573\n",
            "iteration 1965 Loss 0.28737966013932054\n",
            "iteration 1966 Loss 0.287378878748384\n",
            "iteration 1967 Loss 0.28737809736450615\n",
            "iteration 1968 Loss 0.287377315987687\n",
            "iteration 1969 Loss 0.2873765346179262\n",
            "iteration 1970 Loss 0.2873757532552241\n",
            "iteration 1971 Loss 0.28737497189958017\n",
            "iteration 1972 Loss 0.2873741905509947\n",
            "iteration 1973 Loss 0.28737340920946747\n",
            "iteration 1974 Loss 0.28737262787499845\n",
            "iteration 1975 Loss 0.2873718465475876\n",
            "iteration 1976 Loss 0.2873710652272348\n",
            "iteration 1977 Loss 0.28737028391394\n",
            "iteration 1978 Loss 0.2873695026077031\n",
            "iteration 1979 Loss 0.28736872130852414\n",
            "iteration 1980 Loss 0.28736794001640303\n",
            "iteration 1981 Loss 0.2873671587313396\n",
            "iteration 1982 Loss 0.28736637745333393\n",
            "iteration 1983 Loss 0.28736559618238583\n",
            "iteration 1984 Loss 0.28736481491849525\n",
            "iteration 1985 Loss 0.2873640336616623\n",
            "iteration 1986 Loss 0.28736325241188665\n",
            "iteration 1987 Loss 0.2873624711691684\n",
            "iteration 1988 Loss 0.28736168993350747\n",
            "iteration 1989 Loss 0.2873609087049038\n",
            "iteration 1990 Loss 0.2873601274833572\n",
            "iteration 1991 Loss 0.28735934626886783\n",
            "iteration 1992 Loss 0.28735856506143537\n",
            "iteration 1993 Loss 0.28735778386106\n",
            "iteration 1994 Loss 0.2873570026677414\n",
            "iteration 1995 Loss 0.2873562214814797\n",
            "iteration 1996 Loss 0.2873554403022749\n",
            "iteration 1997 Loss 0.2873546591301267\n",
            "iteration 1998 Loss 0.28735387796503514\n",
            "iteration 1999 Loss 0.2873530968070003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydoKkHi2ikcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "e85e3e77-29ac-4804-c32b-b9421ecb10d7"
      },
      "source": [
        "print(theta)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.08100469]\n",
            " [ 0.04768646]\n",
            " [-0.02290339]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjiZHSqukt9N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8c5cd4a7-6f2b-4531-9cbc-85f090ba9492"
      },
      "source": [
        "#testing Phase \n",
        "tmp = np.dot(theta.T,x.T) \n",
        "h2 = sigmoid(tmp)\n",
        "#defining decision boundary\n",
        "h2 = np.where(h2 >= 0.5,1,h2)\n",
        "h2 = np.where(h2 < 0.5,0,h2)\n",
        "#computing error\n",
        "error = np.sum(abs(y-h2))\n",
        "error_rate = (error/m)*100\n",
        "recognition_rate = 100-error_rate\n",
        "print(\"error rate\",error_rate,\"%\")\n",
        "print(\"recognition rate\",recognition_rate,\"%\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error rate 38.75 %\n",
            "recognition rate 61.25 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}